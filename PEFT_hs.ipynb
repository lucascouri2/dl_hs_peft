{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b81c94dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 2.9.0+cu126\n",
      "torch.version.cuda: 12.6\n",
      "cuda available: True\n",
      "gpu count: 1\n",
      "device name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "print(\"torch.__version__:\", torch.__version__)\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu count:\", torch.cuda.device_count())\n",
    "    print(\"device name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
    "os.environ[\"TORCH_CUDNN_V8_API_DISABLED\"] = \"1\"  # evita warning em algumas builds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"dougtrajano/olid-br\")\n",
    "df_train = dataset[\"train\"].to_pandas()\n",
    "df_test = dataset[\"test\"].to_pandas()\n",
    "df_train[\"label\"] = df_train[\"is_offensive\"].apply(lambda x: 1 if x == \"OFF\" else 0)\n",
    "df_test[\"label\"] = df_test[\"is_offensive\"].apply(lambda x: 1 if x == \"OFF\" else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0b2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dataset[\"train\"].to_pandas()\n",
    "df_test = dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15dd54cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_offensive</th>\n",
       "      <th>is_targeted</th>\n",
       "      <th>targeted_type</th>\n",
       "      <th>toxic_spans</th>\n",
       "      <th>health</th>\n",
       "      <th>ideology</th>\n",
       "      <th>insult</th>\n",
       "      <th>lgbtqphobia</th>\n",
       "      <th>other_lifestyle</th>\n",
       "      <th>physical_aspects</th>\n",
       "      <th>profanity_obscene</th>\n",
       "      <th>racism</th>\n",
       "      <th>religious_intolerance</th>\n",
       "      <th>sexism</th>\n",
       "      <th>xenophobia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c779826dc43f460cb18e8429ca443477</td>\n",
       "      <td>Pior do que adolescentezinhas de merda...são p...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a3d7839456ae4258a70298fcf637952e</td>\n",
       "      <td>Podia ter beijo também, pra ver se o homofóbic...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>None</td>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b830374760cc44789f1493c6b228ebc1</td>\n",
       "      <td>o monark não é racista que filha da putagem, j...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "      <td>[36, 37, 38, 39, 40, 41, 42]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73299805588d424fa2905eaf9e616cf1</td>\n",
       "      <td>fudeu minha mãe acabou de chegar em casa e eu ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 72, 73, 74]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>862cf8f819814e47bb4e7f6b4b0afc73</td>\n",
       "      <td>Vergonha,ainda segura a bandeira de um país de...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>None</td>\n",
       "      <td>[58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 6...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  c779826dc43f460cb18e8429ca443477   \n",
       "1  a3d7839456ae4258a70298fcf637952e   \n",
       "2  b830374760cc44789f1493c6b228ebc1   \n",
       "3  73299805588d424fa2905eaf9e616cf1   \n",
       "4  862cf8f819814e47bb4e7f6b4b0afc73   \n",
       "\n",
       "                                                text is_offensive is_targeted  \\\n",
       "0  Pior do que adolescentezinhas de merda...são p...          OFF         UNT   \n",
       "1  Podia ter beijo também, pra ver se o homofóbic...          OFF         UNT   \n",
       "2  o monark não é racista que filha da putagem, j...          OFF         TIN   \n",
       "3  fudeu minha mãe acabou de chegar em casa e eu ...          OFF         TIN   \n",
       "4  Vergonha,ainda segura a bandeira de um país de...          OFF         UNT   \n",
       "\n",
       "  targeted_type                                        toxic_spans  health  \\\n",
       "0          None  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
       "1          None  [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...   False   \n",
       "2           GRP                       [36, 37, 38, 39, 40, 41, 42]   False   \n",
       "3           OTH                     [0, 1, 2, 3, 4, 5, 72, 73, 74]   False   \n",
       "4          None  [58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 6...   False   \n",
       "\n",
       "   ideology  insult  lgbtqphobia  other_lifestyle  physical_aspects  \\\n",
       "0     False    True        False            False             False   \n",
       "1     False    True        False            False             False   \n",
       "2     False    True        False            False             False   \n",
       "3     False    True        False            False             False   \n",
       "4      True    True         True            False             False   \n",
       "\n",
       "   profanity_obscene  racism  religious_intolerance  sexism  xenophobia  \n",
       "0               True   False                  False    True       False  \n",
       "1              False   False                  False   False       False  \n",
       "2              False   False                  False   False       False  \n",
       "3               True   False                  False   False       False  \n",
       "4              False   False                  False   False        True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8987bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"label\"] = df_train[\"is_offensive\"].apply(lambda x: 1 if x == \"OFF\" else 0)\n",
    "df_test[\"label\"] = df_test[\"is_offensive\"].apply(lambda x: 1 if x == \"OFF\" else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb3d0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>functionality</th>\n",
       "      <th>case_id</th>\n",
       "      <th>test_case</th>\n",
       "      <th>label_gold</th>\n",
       "      <th>target_ident</th>\n",
       "      <th>direction</th>\n",
       "      <th>focus_words</th>\n",
       "      <th>focus_lemma</th>\n",
       "      <th>ref_case_id</th>\n",
       "      <th>ref_templ_id</th>\n",
       "      <th>templ_id</th>\n",
       "      <th>case_templ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate women.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>women</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>2</td>\n",
       "      <td>I hate trans people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>trans people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>3</td>\n",
       "      <td>I hate gay people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>gay people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>4</td>\n",
       "      <td>I hate black people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>black people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>5</td>\n",
       "      <td>I hate disabled people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>disabled people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      functionality  case_id                 test_case  \\\n",
       "0           0  derog_neg_emote_h        1            I hate women.    \n",
       "1           1  derog_neg_emote_h        2     I hate trans people.    \n",
       "2           2  derog_neg_emote_h        3       I hate gay people.    \n",
       "3           3  derog_neg_emote_h        4     I hate black people.    \n",
       "4           4  derog_neg_emote_h        5  I hate disabled people.    \n",
       "\n",
       "  label_gold     target_ident direction focus_words focus_lemma  ref_case_id  \\\n",
       "0    hateful            women   general        hate        hate          NaN   \n",
       "1    hateful     trans people   general        hate        hate          NaN   \n",
       "2    hateful       gay people   general        hate        hate          NaN   \n",
       "3    hateful     black people   general        hate        hate          NaN   \n",
       "4    hateful  disabled people   general        hate        hate          NaN   \n",
       "\n",
       "   ref_templ_id  templ_id            case_templ  \n",
       "0           NaN         1  I hate [IDENTITY_P].  \n",
       "1           NaN         1  I hate [IDENTITY_P].  \n",
       "2           NaN         1  I hate [IDENTITY_P].  \n",
       "3           NaN         1  I hate [IDENTITY_P].  \n",
       "4           NaN         1  I hate [IDENTITY_P].  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carrega o arquivo TSV\n",
    "df = pd.read_csv(\"data/all_cases.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8dc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script adaptado para rodar no notebook\n",
    "#!/usr/bin/env python3\n",
    "# subtaskA_local.py\n",
    "# Adapted training script for Subtask A (hate speech detection)\n",
    "# Supports: LoRA (sequence classification) and Prompt Tuning (causal LM)\n",
    "# Designed to run locally with LLaMA 3.2 1B/3B, 4-bit quantization (bitsandbytes)\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PromptTuningConfig,\n",
    "    PromptTuningInit,\n",
    "    get_peft_model,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# ----------------------- Utilities -----------------------\n",
    "\n",
    "def simple_preprocess(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # basic normalizations used in the original pipeline\n",
    "    text = text.replace('&amp;', ' and ')\n",
    "    text = text.replace('&', ' and ')\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_csv(data_path: str, text_col: str = 'text', label_col: str = 'label') -> pd.DataFrame:\n",
    "    df = pd.read_csv(data_path)\n",
    "    assert text_col in df.columns, f\"Column '{text_col}' not found in CSV\"\n",
    "    if label_col not in df.columns:\n",
    "        raise ValueError(f\"Label column '{label_col}' not found in CSV; Subtask A requires labels 0/1\")\n",
    "    # keep only needed columns and drop na\n",
    "    df = df[[text_col, label_col]].dropna().reset_index(drop=True)\n",
    "    df = df.rename(columns={text_col: 'text', label_col: 'label'})\n",
    "    df['text'] = df['text'].astype(str).map(simple_preprocess)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_dataset_from_bib(dataset_name: str = \"dougtrajano/olid-br\", text_col: str = 'text', label_col: str = 'label') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    dataset = load_dataset(dataset_name) \n",
    "    df_train = dataset[\"train\"].to_pandas()\n",
    "    df_test = dataset[\"test\"].to_pandas()\n",
    "\n",
    "    df_train[\"label\"] = df_train[\"is_offensive\"].apply(lambda x: 1 if x == \"OFF\" else 0)\n",
    "    df_test[\"label\"] = df_test[\"is_offensive\"].apply(lambda x: 1 if x == \"OFF\" else 0)\n",
    "\n",
    "    # keep only needed columns and drop na\n",
    "    df_train = df_train[[text_col, label_col]].dropna().reset_index(drop=True)\n",
    "    df_train = df_train.rename(columns={text_col: 'text', label_col: 'label'})\n",
    "    df_train['text'] = df_train['text'].astype(str).map(simple_preprocess)\n",
    "    df_train['label'] = df_train['label'].astype(int)\n",
    "\n",
    "    # keep only needed columns and drop na\n",
    "    df_test = df_test[[text_col, label_col]].dropna().reset_index(drop=True)\n",
    "    df_test = df_test.rename(columns={text_col: 'text', label_col: 'label'})\n",
    "    df_test['text'] = df_test['text'].astype(str).map(simple_preprocess)\n",
    "    df_test['label'] = df_test['label'].astype(int)\n",
    "    return df_train, df_test\n",
    "\n",
    "def load_synthetic(path: str, text_col: str = 'text', label_col: str = 'label') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = pd.read_csv(\"data/all_cases.csv\") \n",
    "    \n",
    "    df['text'] = df['test_case']\n",
    "    df[\"label\"] = df[\"label_gold\"].apply(lambda x: 1 if x == \"hateful\" else 0)\n",
    "\n",
    "    df = df[[text_col, label_col]].dropna().reset_index(drop=True)\n",
    "    df = df.rename(columns={text_col: 'text', label_col: 'label'})\n",
    "    df['text'] = df['text'].astype(str).map(simple_preprocess)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size = 0.2, random_state=42, shuffle=True)\n",
    "    df_train, df_val = train_test_split(df_train, test_size = 0.2, random_state=42, shuffle=True)\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "# ----------------------- LoRA (Sequence Classification) -----------------------\n",
    "\n",
    "def train_lora(\n",
    "    model_name: str,\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    output_dir: str,\n",
    "    batch_size: int = 4,\n",
    "    num_epochs: int = 3,\n",
    "    learning_rate: float = 1e-4,\n",
    "    lora_r: int = 16,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    use_4bit: bool = True,\n",
    "):\n",
    "    print('*** Training with LoRA (Sequence Classification)')\n",
    "\n",
    "    # bitsandbytes config for 4-bit\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype='float32',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    # Load tokenizer and model for seq classification\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        device_map='auto',\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # Add LoRA PEFT\n",
    "    peft_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=['q_proj', 'v_proj'],\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias='none',\n",
    "        task_type='SEQ_CLS',\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Tokenize datasets\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    val_ds = Dataset.from_pandas(val_df)\n",
    "    train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "    val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "    train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_f1',\n",
    "        greater_is_better=True,\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        logits = pred.predictions\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        labels = pred.label_ids\n",
    "        return {\n",
    "            'accuracy': accuracy_score(labels, preds),\n",
    "            'f1': f1_score(labels, preds, average='macro'),\n",
    "            'precision': precision_score(labels, preds, zero_division=0),\n",
    "            'recall': recall_score(labels, preds, zero_division=0),\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    print('--- Evaluation on validation set ---')\n",
    "    preds = trainer.predict(val_ds)\n",
    "    print(preds.metrics)\n",
    "    return trainer, tokenizer\n",
    "\n",
    "\n",
    "# ----------------------- Prompt Tuning (Causal LM) -----------------------\n",
    "\n",
    "def train_prompt_tuning(\n",
    "    model_name: str,\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    output_dir: str,\n",
    "    batch_size: int = 4,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 5e-4,\n",
    "    num_virtual_tokens: int = 40,\n",
    "    use_4bit: bool = False,   # IMPORTANT: default False for stability with prompt tuning\n",
    "):\n",
    "    \"\"\"\n",
    "    Prompt tuning training loop (improved):\n",
    "     - Uses an explicit instruction template\n",
    "     - Defaults to no 4-bit quantization for stability (can be enabled)\n",
    "     - Performs batch generation evaluation, extracting only generated tokens\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"  # ensure accelerate not forcing AMP\n",
    "\n",
    "    print('*** Training with Prompt Tuning (Causal LM) - improved version ***')\n",
    "    print(f\"Model: {model_name} | num_virtual_tokens: {num_virtual_tokens} | use_4bit: {use_4bit}\")\n",
    "\n",
    "    # BitsAndBytes config (used only if use_4bit=True)\n",
    "    bnb_config = None\n",
    "    if use_4bit:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_compute_dtype='float16',\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n",
    "    # Ensure padding token set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    #tokenizer.padding_side = getattr(tokenizer, \"padding_side\", \"right\")\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    # Load model (CausalLM)\n",
    "    model_kwargs = dict(device_map='auto', trust_remote_code=True)\n",
    "    if bnb_config is not None:\n",
    "        model_kwargs['quantization_config'] = bnb_config\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "\n",
    "    # Prompt tuning config (soft prompt initialized from text)\n",
    "    prompt_cfg = PromptTuningConfig(\n",
    "        task_type='CAUSAL_LM',\n",
    "        prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "        num_virtual_tokens=num_virtual_tokens,\n",
    "        prompt_tuning_init_text='[INST] Your task: classify if the text contains hate speech (0=no, 1=yes). [/INST]',\n",
    "        tokenizer_name_or_path=model_name,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, prompt_cfg)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Instruction template (explicit; helps model understand the task)\n",
    "    template = \"[INST] Classify whether the following sentence contains hate speech. Answer with a single token '0' (no) or '1' (yes).\\n\\nText: {text} [/INST]\"\n",
    "\n",
    "    # Build training-style input sequences where label is target text\n",
    "    def build_example(row):\n",
    "        inp = template.format(text=row['text'])\n",
    "        tgt = str(int(row['label']))\n",
    "        return {'input_text': inp, 'target_text': tgt}\n",
    "\n",
    "    train_examples = [build_example(r) for _, r in train_df.iterrows()]\n",
    "    val_examples = [build_example(r) for _, r in val_df.iterrows()]\n",
    "\n",
    "    # Tokenize with text_target to enable causal LM labels (Trainer calculates loss)\n",
    "    def tokenize_fn(examples):\n",
    "        model_inputs = tokenizer(\n",
    "            examples['input_text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "        )\n",
    "\n",
    "        # Tokenizar o alvo como texto\n",
    "        labels = tokenizer(\n",
    "            examples['target_text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=8,\n",
    "        )['input_ids']\n",
    "\n",
    "        # Preenche os labels para o mesmo comprimento de input_ids com -100\n",
    "        full_labels = []\n",
    "        for i in range(len(model_inputs['input_ids'])):\n",
    "            lbl = labels[i]\n",
    "            # cria vetor do mesmo tamanho que input_ids preenchido com -100\n",
    "            padded = [-100] * len(model_inputs['input_ids'][i])\n",
    "            # substitui os últimos tokens pelos rótulos (curtos)\n",
    "            padded[-len(lbl):] = lbl\n",
    "            full_labels.append(padded)\n",
    "\n",
    "        model_inputs['labels'] = full_labels\n",
    "        return model_inputs\n",
    "        # examples is a dict with lists: 'input_text' and 'target_text'\n",
    "        #return tokenizer(examples['input_text'], text_target=examples['target_text'],\n",
    "        #                 padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "    train_ds = Dataset.from_list(train_examples)\n",
    "    val_ds = Dataset.from_list(val_examples)\n",
    "    train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=['input_text', 'target_text'])\n",
    "    val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=['input_text', 'target_text'])\n",
    "    train_ds.set_format(type='torch')\n",
    "    val_ds.set_format(type='torch')\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n",
    "\n",
    "    # Training args - NO AMP (fp16/bf16) for prompt tuning unless you know what you do\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        greater_is_better=False,\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "    # We will not use a compute_metrics in Trainer for generation-based metric here (too noisy).\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    train_result = trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Manual batch evaluation using generation (faster and controlled)\n",
    "    print('\\n--- Manual evaluation of Prompt-tuned model on validation set (generation + parse) ---')\n",
    "    model.eval()\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    val_texts = [template.format(text=t) for t in val_df['text'].tolist()]\n",
    "    val_labels = [int(x) for x in val_df['label'].tolist()]\n",
    "\n",
    "    # Batch generation\n",
    "    preds = []\n",
    "    batch_size_gen = max(1, 8)  # generation batch size (you can increase if GPU allows)\n",
    "    for i in range(0, len(val_texts), batch_size_gen):\n",
    "        batch_texts = val_texts[i:i+batch_size_gen]\n",
    "        # tokenize input only (no target) for generation\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs.get('attention_mask', None)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "        # generate deterministically (greedy)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=8,   # allow some room to generate label or small text\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_k=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                early_stopping=True,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "        # For each sample, extract only the generated portion (after input length)\n",
    "        for b_idx in range(out.shape[0]):\n",
    "            generated_ids = out[b_idx]\n",
    "            # slice off the input prefix\n",
    "            in_len = input_ids.shape[1]  # note: this is same for batch due to padding\n",
    "            # However when padded, the model output includes tokens for padding; safer to find the first occurrence of eos or decode and remove input text\n",
    "            # Strategy: decode whole output then remove the decoded input prefix if present\n",
    "            decoded_full = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "            # Try to remove the input text from decoded_full if present (best-effort)\n",
    "            prefix = tokenizer.decode(input_ids[b_idx], skip_special_tokens=True).strip()\n",
    "            if decoded_full.startswith(prefix):\n",
    "                generated_text = decoded_full[len(prefix):].strip()\n",
    "            else:\n",
    "                # fallback: take last tokens up to a short string\n",
    "                generated_text = decoded_full\n",
    "\n",
    "            # Parse first occurrence of 0 or 1 in generated_text\n",
    "            m = re.search(r\"\\b[01]\\b\", generated_text)\n",
    "            if m:\n",
    "                pred = int(m.group())\n",
    "            else:\n",
    "                # fallback: sometimes model outputs 'No'/'Yes' or words; map heuristics\n",
    "                gen_lower = generated_text.lower()\n",
    "                if gen_lower.startswith('no') or 'no' in gen_lower.split():\n",
    "                    pred = 0\n",
    "                elif gen_lower.startswith('yes') or 'yes' in gen_lower.split():\n",
    "                    pred = 1\n",
    "                else:\n",
    "                    # default to most frequent class in training (safe fallback)\n",
    "                    pred = int(round(np.mean(train_df['label'].values)))  # majority class fallback\n",
    "            preds.append(pred)\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(val_labels, preds, digits=4))\n",
    "\n",
    "    return trainer, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def main(method='lora', model_name='meta-llama/Llama-3.2-1B', output_dir='output', batch_size=4,\n",
    "         epochs=3, lr=1e-4):\n",
    "    print('Loading CSV..')\n",
    "\n",
    "    #olidbr\n",
    "    #train_df, test_df = load_dataset_from_bib()\n",
    "    #train_df, val_df = train_test_split(train_df, test_size = 0.2, random_state=42, shuffle=True)\n",
    "    \n",
    "    # synthetic\n",
    "    train_df, val_df, test_df = load_synthetic(path=\"data/all_cases.csv\")\n",
    "\n",
    "    print(f\"Train size: {len(train_df)} - Val size: {len(val_df)} - Test size: {len(test_df)}\")\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    if method == 'lora':\n",
    "        trainer, tokenizer = train_lora(\n",
    "            model_name=model_name,\n",
    "            train_df=train_df,\n",
    "            val_df=val_df,\n",
    "            output_dir=output_dir,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=epochs,\n",
    "            learning_rate=lr,\n",
    "            lora_r=16,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.05,\n",
    "            use_4bit=True,\n",
    "        )\n",
    "    else:\n",
    "        trainer, tokenizer = train_prompt_tuning(\n",
    "            model_name=model_name,\n",
    "            train_df=train_df,\n",
    "            val_df=val_df,\n",
    "            output_dir=output_dir,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=epochs,\n",
    "            learning_rate=lr,\n",
    "            num_virtual_tokens=60,\n",
    "            use_4bit=True,\n",
    "        )\n",
    "\n",
    "    print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5741ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV..\n",
      "Train size: 2496 - Val size: 624 - Test size: 781\n",
      "*** Training with LoRA (Sequence Classification)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfb9aac3b92471c8004412f6b9b0c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lucas\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cf0b428a1c4687a6e6532854943180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c685cba84fec4f7dabbac7a56a03fb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17922d31ad074307aeeeaefd5fc01240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc6 in position 8: invalid continuation byte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afbbfaa84554b1987c866daae066100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9925874dee455dad59c33da712e6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5ce672eba54a8790b774416f54e11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f26cb9fb8a42768ac6cd3a0df606c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8ccf0c440645c89b283cc1986ab727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,593,664 || all params: 3,217,349,632 || trainable%: 0.1428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0b3d8ae8004430b329035026b83de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712e7dc9f2154c62b93b265e4032a732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_17040\\2232452193.py:193: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1133' max='1872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1133/1872 45:01 < 29:24, 0.42 it/s, Epoch 1.81/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.017471</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>0.996057</td>\n",
       "      <td>0.995536</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlora\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.2-3B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 454\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(method, model_name, output_dir, batch_size, epochs, lr)\u001b[0m\n\u001b[0;32m    451\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlora\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 454\u001b[0m     trainer, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lora\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_r\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m     trainer, tokenizer \u001b[38;5;241m=\u001b[39m train_prompt_tuning(\n\u001b[0;32m    469\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    470\u001b[0m         train_df\u001b[38;5;241m=\u001b[39mtrain_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    477\u001b[0m         use_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    478\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[3], line 203\u001b[0m, in \u001b[0;36mtrain_lora\u001b[1;34m(model_name, train_df, val_df, output_dir, batch_size, num_epochs, learning_rate, lora_r, lora_alpha, lora_dropout, use_4bit)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy_score(labels, preds),\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_score(labels, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: precision_score(labels, preds, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall_score(labels, preds, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    191\u001b[0m     }\n\u001b[0;32m    193\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    194\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    195\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    201\u001b[0m )\n\u001b[1;32m--> 203\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(output_dir)\n\u001b[0;32m    205\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\transformers\\trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(method='lora', model_name='meta-llama/Llama-3.2-3B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8cc89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV..\n",
      "Train size: 2496 - Val size: 624 - Test size: 781\n",
      "*** Training with Prompt Tuning (Causal LM) - improved version ***\n",
      "Model: meta-llama/Llama-3.2-3B | num_virtual_tokens: 60 | use_4bit: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc6 in position 8: invalid continuation byte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8a7006b8484c52b4d952579a50e036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aeace33468a444db1184b198e91141c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lucas\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 184,320 || all params: 3,212,934,144 || trainable%: 0.0057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b883364f336143a4bd08b4ec25287e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576cea3fe0e5444cb2b26aeea1f88a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_22580\\2232452193.py:350: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1872' max='1872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1872/1872 36:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.043608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.019144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.012792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Manual evaluation of Prompt-tuned model on validation set (generation + parse) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\peft\\peft_model.py:2066: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       178\n",
      "           1     0.7147    1.0000    0.8336       446\n",
      "\n",
      "    accuracy                         0.7147       624\n",
      "   macro avg     0.3574    0.5000    0.4168       624\n",
      "weighted avg     0.5109    0.7147    0.5958       624\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "main(method='prompt', model_name='meta-llama/Llama-3.2-3B', epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl_projeto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
