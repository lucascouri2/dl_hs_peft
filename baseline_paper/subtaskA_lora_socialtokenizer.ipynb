{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f74565f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] A sintaxe do nome do arquivo, do nome do diretório ou do rótulo do volume está incorreta: '<frozen importlib._bootstrap>'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m notebook_login \n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\__init__.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m termsetup, termlog, termerror, termwarn\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Configure the logger as early as possible for consistent behavior.\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wb_logging \u001b[38;5;28;01mas\u001b[39;00m _wb_logging\n\u001b[0;32m     23\u001b[0m _wb_logging\u001b[38;5;241m.\u001b[39mconfigure_wandb_logger()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\sdk\\__init__.py:25\u001b[0m\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSettings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_helper \u001b[38;5;28;01mas\u001b[39;00m helper\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Artifact\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_alerts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlertLevel\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\sdk\\artifacts\\artifact.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NonNegativeInt\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data_types, env\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_iterutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m one, unique_list\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_json\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\data_types.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This module defines data types for logging rich, interactive visualizations to W&B.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mData types include common media types, like images, audio, and videos,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mand upload them to the W&B server.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_types\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_types\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_types\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmedia\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchableMedia, Media\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_types\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_types\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwb_value\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WBValue\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\sdk\\data_types\\audio.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _dtypes\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MEDIA_TMP\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_types\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmedia\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchableMedia\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\sdk\\data_types\\base_types\\media.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m filesystem\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogicalPath\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwb_value\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WBValue\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\sdk\\data_types\\base_types\\wb_value.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, ClassVar, Dict, List, Optional, Type, Union\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_setup\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Artifact\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_DIR\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m asyncio_manager, import_hooks, wb_logging\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_settings\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_util, server\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\sdk\\wandb_settings.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UsageError\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_settings_pb2\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apikey, credentials, ipython\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgitlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GitRepo\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_moment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunMoment\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\sdk\\lib\\apikey.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NETRC_FILES, get_netrc_auth\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InternalApi\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m term\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m url_registry\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\apis\\__init__.py:46\u001b[0m\n\u001b[0;32m     43\u001b[0m reset_path \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mvendor_setup()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m PublicApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     48\u001b[0m reset_path()\n\u001b[0;32m     50\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInternalApi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPublicApi\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\apis\\public\\__init__.py:44\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApi\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryingClient\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# doc:exclude\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     41\u001b[0m )\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api, RetryingClient, requests\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     46\u001b[0m     ArtifactCollection,\n\u001b[0;32m     47\u001b[0m     ArtifactCollections,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     RunArtifacts,\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautomations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Automations\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\apis\\public\\api.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_exceptions\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconst\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RETRY_TIMEDELTA\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistries\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Registries, Registry\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fetch_org_entity_from_organization\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     40\u001b[0m     PathType,\n\u001b[0;32m     41\u001b[0m     fetch_org_from_settings_or_entity,\n\u001b[0;32m     42\u001b[0m     gql_compat,\n\u001b[0;32m     43\u001b[0m     parse_org_from_registry_path,\n\u001b[0;32m     44\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\apis\\public\\registries\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# doc:exclude\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegistries\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# doc:exclude\u001b[39;00m\n\u001b[0;32m      4\u001b[0m ]\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistries_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Registries\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Registry\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\apis\\public\\registries\\registries_search.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_analytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracked\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaginator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Paginator\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArtifactCollection\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gql_compat\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_generated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     FETCH_REGISTRIES_GQL,\n\u001b[0;32m     18\u001b[0m     REGISTRY_COLLECTIONS_GQL,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     RegistryVersions,\n\u001b[0;32m     23\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\apis\\public\\artifacts.py:71\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_gqlutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m omit_artifact_fields\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArtifactCollectionData\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpagination\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     72\u001b[0m     ArtifactCollectionConnection,\n\u001b[0;32m     73\u001b[0m     ArtifactFileConnection,\n\u001b[0;32m     74\u001b[0m     ArtifactTypeConnection,\n\u001b[0;32m     75\u001b[0m     RunArtifactConnection,\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FullArtifactPath, validate_artifact_type\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\wandb\\sdk\\artifacts\\_models\\pagination.py:25\u001b[0m\n\u001b[0;32m     21\u001b[0m ArtifactFileConnection \u001b[38;5;241m=\u001b[39m Connection[FileFragment]\n\u001b[0;32m     23\u001b[0m RunArtifactConnection \u001b[38;5;241m=\u001b[39m ConnectionWithTotal[ArtifactFragment]\n\u001b[1;32m---> 25\u001b[0m RegistryConnection \u001b[38;5;241m=\u001b[39m \u001b[43mConnection\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRegistryFragment\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     26\u001b[0m RegistryCollectionConnection \u001b[38;5;241m=\u001b[39m ConnectionWithTotal[RegistryCollectionFragment]\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\main.py:949\u001b[0m, in \u001b[0;36mBaseModel.__class_getitem__\u001b[1;34m(cls, typevar_values)\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation:\n\u001b[0;32m    945\u001b[0m             \u001b[38;5;66;03m# It's okay if it fails, it just means there are still undefined types\u001b[39;00m\n\u001b[0;32m    946\u001b[0m             \u001b[38;5;66;03m# that could be evaluated later.\u001b[39;00m\n\u001b[0;32m    947\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 949\u001b[0m         submodel \u001b[38;5;241m=\u001b[39m \u001b[43m_generics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_generic_submodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m         _generics\u001b[38;5;241m.\u001b[39mset_cached_generic_type(\u001b[38;5;28mcls\u001b[39m, typevar_values, submodel, origin, args)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m submodel\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_generics.py:127\u001b[0m, in \u001b[0;36mcreate_generic_submodel\u001b[1;34m(model_name, origin, args, params)\u001b[0m\n\u001b[0;32m    125\u001b[0m meta, ns, kwds \u001b[38;5;241m=\u001b[39m prepare_class(model_name, bases)\n\u001b[0;32m    126\u001b[0m namespace\u001b[38;5;241m.\u001b[39mupdate(ns)\n\u001b[1;32m--> 127\u001b[0m created_model \u001b[38;5;241m=\u001b[39m meta(\n\u001b[0;32m    128\u001b[0m     model_name,\n\u001b[0;32m    129\u001b[0m     bases,\n\u001b[0;32m    130\u001b[0m     namespace,\n\u001b[0;32m    131\u001b[0m     __pydantic_generic_metadata__\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m: origin,\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m: args,\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m: params,\n\u001b[0;32m    135\u001b[0m     },\n\u001b[0;32m    136\u001b[0m     __pydantic_reset_parent_namespace__\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m    138\u001b[0m )\n\u001b[0;32m    140\u001b[0m model_module, called_globally \u001b[38;5;241m=\u001b[39m _get_caller_frame_info(depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m called_globally:  \u001b[38;5;66;03m# create global reference and therefore allow pickling\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py:242\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m     parent_namespace \u001b[38;5;241m=\u001b[39m unpack_lenient_weakvaluedict(parent_namespace)\n\u001b[0;32m    240\u001b[0m ns_resolver \u001b[38;5;241m=\u001b[39m NsResolver(parent_namespace\u001b[38;5;241m=\u001b[39mparent_namespace)\n\u001b[1;32m--> 242\u001b[0m \u001b[43mset_model_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mns_resolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# This is also set in `complete_model_class()`, after schema gen because they are recreated.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# We set them here as well for backwards compatibility:\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_computed_fields__ \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    247\u001b[0m     k: v\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_decorators__\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    248\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py:566\u001b[0m, in \u001b[0;36mset_model_fields\u001b[1;34m(cls, config_wrapper, ns_resolver)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Collect and set `cls.__pydantic_fields__` and `cls.__class_vars__`.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \n\u001b[0;32m    560\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;124;03m    ns_resolver: Namespace resolver to use when getting model annotations.\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    565\u001b[0m typevars_map \u001b[38;5;241m=\u001b[39m get_model_typevars_map(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 566\u001b[0m fields, class_vars \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_model_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypevars_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypevars_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_fields__ \u001b[38;5;241m=\u001b[39m fields\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__class_vars__\u001b[38;5;241m.\u001b[39mupdate(class_vars)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_fields.py:408\u001b[0m, in \u001b[0;36mcollect_model_fields\u001b[1;34m(cls, config_wrapper, ns_resolver, typevars_map)\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39m_complete:\n\u001b[1;32m--> 408\u001b[0m             \u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_typevars_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypevars_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper\u001b[38;5;241m.\u001b[39muse_attribute_docstrings:\n\u001b[0;32m    411\u001b[0m     _update_fields_from_docstrings(\u001b[38;5;28mcls\u001b[39m, fields)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\fields.py:799\u001b[0m, in \u001b[0;36mFieldInfo.apply_typevars_map\u001b[1;34m(self, typevars_map, globalns, localns)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_typevars_map\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     typevars_map: Mapping[TypeVar, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    781\u001b[0m     globalns: GlobalsNamespace \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    782\u001b[0m     localns: MappingNamespace \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    783\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    784\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply a `typevars_map` to the annotation.\u001b[39;00m\n\u001b[0;32m    785\u001b[0m \n\u001b[0;32m    786\u001b[0m \u001b[38;5;124;03m    This method is used when analyzing parametrized generic types to replace typevars with their concrete types.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;124;03m            their concrete types.\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m     annotation \u001b[38;5;241m=\u001b[39m \u001b[43m_generics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypevars_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m     annotation, evaluated \u001b[38;5;241m=\u001b[39m _typing_extra\u001b[38;5;241m.\u001b[39mtry_eval_type(annotation, globalns, localns)\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotation \u001b[38;5;241m=\u001b[39m annotation\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_generics.py:281\u001b[0m, in \u001b[0;36mreplace_types\u001b[1;34m(type_, type_map)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# Having type args is a good indicator that this is a typing special form\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# instance or a generic alias of some sort.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_args:\n\u001b[1;32m--> 281\u001b[0m     resolved_type_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplace_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtype_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m all_identical(type_args, resolved_type_args):\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;66;03m# If all arguments are the same, there is no need to modify the\u001b[39;00m\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;66;03m# type or create a new object at all\u001b[39;00m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m type_\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_generics.py:281\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# Having type args is a good indicator that this is a typing special form\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# instance or a generic alias of some sort.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_args:\n\u001b[1;32m--> 281\u001b[0m     resolved_type_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mreplace_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m type_args)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m all_identical(type_args, resolved_type_args):\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;66;03m# If all arguments are the same, there is no need to modify the\u001b[39;00m\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;66;03m# type or create a new object at all\u001b[39;00m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m type_\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_generics.py:327\u001b[0m, in \u001b[0;36mreplace_types\u001b[1;34m(type_, type_map)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m all_identical(parameters, resolved_type_args):\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m type_\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtype_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mresolved_type_args\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Handle special case for typehints that can have lists as arguments.\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# `typing.Callable[[int, str], int]` is an example for this.\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(type_, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\main.py:949\u001b[0m, in \u001b[0;36mBaseModel.__class_getitem__\u001b[1;34m(cls, typevar_values)\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation:\n\u001b[0;32m    945\u001b[0m             \u001b[38;5;66;03m# It's okay if it fails, it just means there are still undefined types\u001b[39;00m\n\u001b[0;32m    946\u001b[0m             \u001b[38;5;66;03m# that could be evaluated later.\u001b[39;00m\n\u001b[0;32m    947\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 949\u001b[0m         submodel \u001b[38;5;241m=\u001b[39m \u001b[43m_generics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_generic_submodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m         _generics\u001b[38;5;241m.\u001b[39mset_cached_generic_type(\u001b[38;5;28mcls\u001b[39m, typevar_values, submodel, origin, args)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m submodel\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_generics.py:127\u001b[0m, in \u001b[0;36mcreate_generic_submodel\u001b[1;34m(model_name, origin, args, params)\u001b[0m\n\u001b[0;32m    125\u001b[0m meta, ns, kwds \u001b[38;5;241m=\u001b[39m prepare_class(model_name, bases)\n\u001b[0;32m    126\u001b[0m namespace\u001b[38;5;241m.\u001b[39mupdate(ns)\n\u001b[1;32m--> 127\u001b[0m created_model \u001b[38;5;241m=\u001b[39m meta(\n\u001b[0;32m    128\u001b[0m     model_name,\n\u001b[0;32m    129\u001b[0m     bases,\n\u001b[0;32m    130\u001b[0m     namespace,\n\u001b[0;32m    131\u001b[0m     __pydantic_generic_metadata__\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m: origin,\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m: args,\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m: params,\n\u001b[0;32m    135\u001b[0m     },\n\u001b[0;32m    136\u001b[0m     __pydantic_reset_parent_namespace__\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m    138\u001b[0m )\n\u001b[0;32m    140\u001b[0m model_module, called_globally \u001b[38;5;241m=\u001b[39m _get_caller_frame_info(depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m called_globally:  \u001b[38;5;66;03m# create global reference and therefore allow pickling\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py:242\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m     parent_namespace \u001b[38;5;241m=\u001b[39m unpack_lenient_weakvaluedict(parent_namespace)\n\u001b[0;32m    240\u001b[0m ns_resolver \u001b[38;5;241m=\u001b[39m NsResolver(parent_namespace\u001b[38;5;241m=\u001b[39mparent_namespace)\n\u001b[1;32m--> 242\u001b[0m \u001b[43mset_model_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mns_resolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# This is also set in `complete_model_class()`, after schema gen because they are recreated.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# We set them here as well for backwards compatibility:\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_computed_fields__ \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    247\u001b[0m     k: v\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_decorators__\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    248\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py:566\u001b[0m, in \u001b[0;36mset_model_fields\u001b[1;34m(cls, config_wrapper, ns_resolver)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Collect and set `cls.__pydantic_fields__` and `cls.__class_vars__`.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \n\u001b[0;32m    560\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;124;03m    ns_resolver: Namespace resolver to use when getting model annotations.\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    565\u001b[0m typevars_map \u001b[38;5;241m=\u001b[39m get_model_typevars_map(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 566\u001b[0m fields, class_vars \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_model_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypevars_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypevars_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_fields__ \u001b[38;5;241m=\u001b[39m fields\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__class_vars__\u001b[38;5;241m.\u001b[39mupdate(class_vars)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_fields.py:411\u001b[0m, in \u001b[0;36mcollect_model_fields\u001b[1;34m(cls, config_wrapper, ns_resolver, typevars_map)\u001b[0m\n\u001b[0;32m    408\u001b[0m             field\u001b[38;5;241m.\u001b[39mapply_typevars_map(typevars_map)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper\u001b[38;5;241m.\u001b[39muse_attribute_docstrings:\n\u001b[1;32m--> 411\u001b[0m     \u001b[43m_update_fields_from_docstrings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fields, class_vars\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_fields.py:114\u001b[0m, in \u001b[0;36m_update_fields_from_docstrings\u001b[1;34m(cls, fields, use_inspect)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_fields_from_docstrings\u001b[39m(\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[Any], fields: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, FieldInfo], use_inspect: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     fields_docs \u001b[38;5;241m=\u001b[39m \u001b[43mextract_docstrings_from_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_inspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_inspect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ann_name, field_info \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m field_info\u001b[38;5;241m.\u001b[39mdescription \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ann_name \u001b[38;5;129;01min\u001b[39;00m fields_docs:\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_docs_extraction.py:104\u001b[0m, in \u001b[0;36mextract_docstrings_from_cls\u001b[1;34m(cls, use_inspect)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# TODO remove this implementation when we drop support for Python 3.12:\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_source_from_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m source:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\pydantic\\_internal\\_docs_extraction.py:56\u001b[0m, in \u001b[0;36m_extract_source_from_frame\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m     53\u001b[0m frame \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mcurrentframe()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m     57\u001b[0m         lnum \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_lineno\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\inspect.py:861\u001b[0m, in \u001b[0;36mgetmodule\u001b[1;34m(object, _filename)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;66;03m# Try the cache again with the absolute file name\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mgetabsfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\inspect.py:844\u001b[0m, in \u001b[0;36mgetabsfile\u001b[1;34m(object, _filename)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return an absolute path to the source or compiled file for an object.\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \n\u001b[0;32m    841\u001b[0m \u001b[38;5;124;03mThe idea is for each object to have a unique origin, so this routine\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;124;03mnormalizes the result as much as possible.\"\"\"\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 844\u001b[0m     _filename \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m getfile(\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormcase(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_filename))\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\inspect.py:826\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(filename\u001b[38;5;241m.\u001b[39mendswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    824\u001b[0m              importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[0;32m    828\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import wandb \n",
    "from copy import deepcopy\n",
    "from huggingface_hub import notebook_login \n",
    "from datasets import load_dataset, concatenate_datasets \n",
    "from datasets import Dataset  \n",
    "import warnings\n",
    "from transformers.utils import logging\n",
    "import evaluate\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wordsegment as ws\n",
    "import seaborn as sns\n",
    "import emoji\n",
    "import random\n",
    "import re\n",
    "import glob\n",
    "from skimpy import skim\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback, DataCollatorWithPadding\n",
    "from huggingface_hub import login\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score,  matthews_corrcoef\n",
    "from typing import Callable\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "import torchvision\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    PeftType,\n",
    "    PromptEncoderConfig,\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    PromptTuningInit,\n",
    "    PromptTuningConfig,\n",
    "    TaskType,\n",
    "    PeftType,\n",
    "    PromptEncoder,\n",
    "    PrefixTuningConfig)\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback, \n",
    "    pipeline,\n",
    "    AutoConfig, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForImageClassification,\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer)\n",
    "\n",
    "#global tokenizer\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity(logging.CRITICAL) \n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "  info = torch.cuda.get_device_properties(i)\n",
    "  print(f\"CUDA:{i} {info.name}, {info.total_memory / 1024 ** 2}MB\")\n",
    "\n",
    "# Set up GPU for Training\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "  print('Device name:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  print('No GPU available, using the CPU instead.')\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"  # segurança extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff9448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "================ === teste bloomz en === ================\n",
      "\n",
      "Output directory: /data/outputs/\n",
      "Model Name: meta-llama/Llama-3.2-1B\n",
      "Output Directory: /data/outputs/meta-llama/Llama-3.2-1B/\n",
      "\n",
      "Checking that the necessary paths exist...\n",
      "Directory /data/ already exists!\n",
      "Directory /data/outputs/ already exists!\n",
      "Directory /data/results/ already exists!\n",
      "Directory /data/outputs/meta-llama/Llama-3.2-1B/ already exists!\n",
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n",
      "The prompt contains 35 tokens.\n",
      "Current file opened:  ['data/en_train.csv\\\\data/en_train.csv']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
       "│ <span style=\"font-style: italic\">         Data Summary         </span> <span style=\"font-style: italic\">      Data Types       </span>                                                          │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n",
       "│ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Dataframe         </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Values </span>┃ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Column Type </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Count </span>┃                                                          │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n",
       "│ │ Number of rows    │ 2496   │ │ string      │ 1     │                                                          │\n",
       "│ │ Number of columns │ 2      │ │ int64       │ 1     │                                                          │\n",
       "│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n",
       "│ <span style=\"font-style: italic\">                                                    number                                                    </span>  │\n",
       "│ ┏━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┓  │\n",
       "│ ┃<span style=\"font-weight: bold\"> column     </span>┃<span style=\"font-weight: bold\"> NA   </span>┃<span style=\"font-weight: bold\"> NA %    </span>┃<span style=\"font-weight: bold\"> mean       </span>┃<span style=\"font-weight: bold\"> sd         </span>┃<span style=\"font-weight: bold\"> p0   </span>┃<span style=\"font-weight: bold\"> p25    </span>┃<span style=\"font-weight: bold\"> p50   </span>┃<span style=\"font-weight: bold\"> p75   </span>┃<span style=\"font-weight: bold\"> p100    </span>┃<span style=\"font-weight: bold\"> hist     </span>┃  │\n",
       "│ ┡━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━┩  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">label     </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    0.6707</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    0.4701</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">     0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      1</span> │ <span style=\"color: #008000; text-decoration-color: #008000\"> ▃    ▇ </span> │  │\n",
       "│ └────────────┴──────┴─────────┴────────────┴────────────┴──────┴────────┴───────┴───────┴─────────┴──────────┘  │\n",
       "│ <span style=\"font-style: italic\">                                                    string                                                    </span>  │\n",
       "│ ┏━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓  │\n",
       "│ ┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\">    </span>┃<span style=\"font-weight: bold\">      </span>┃<span style=\"font-weight: bold\">            </span>┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\">            </span>┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\"> chars per  </span>┃<span style=\"font-weight: bold\"> words per </span>┃<span style=\"font-weight: bold\"> total      </span>┃  │\n",
       "│ ┃<span style=\"font-weight: bold\"> column </span>┃<span style=\"font-weight: bold\"> NA </span>┃<span style=\"font-weight: bold\"> NA % </span>┃<span style=\"font-weight: bold\"> shortest   </span>┃<span style=\"font-weight: bold\"> longest   </span>┃<span style=\"font-weight: bold\"> min        </span>┃<span style=\"font-weight: bold\"> max       </span>┃<span style=\"font-weight: bold\"> row        </span>┃<span style=\"font-weight: bold\"> row       </span>┃<span style=\"font-weight: bold\"> words      </span>┃  │\n",
       "│ ┡━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">text  </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">I hate    </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">When will</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">\"Muslims  </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Your     </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      47.2</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      8.8</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">     22086</span> │  │\n",
       "│ │        │    │      │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">you.      </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">it       </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">are a     </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">trans    </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">finally  </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">cancer\" is</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">livs are </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">be clear </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">such a    </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">worthless</span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">to people</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">dehumanisi</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">.        </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">that it's</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">ng thing  </span> │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">in a     </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">to say.   </span> │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">disabled </span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">person's </span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">nature to</span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">be an    </span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">annoying </span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">prick?   </span> │            │           │            │           │            │  │\n",
       "│ └────────┴────┴──────┴────────────┴───────────┴────────────┴───────────┴────────────┴───────────┴────────────┘  │\n",
       "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
       "│ \u001b[3m         Data Summary         \u001b[0m \u001b[3m      Data Types       \u001b[0m                                                          │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n",
       "│ ┃\u001b[1;36m \u001b[0m\u001b[1;36mDataframe        \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValues\u001b[0m\u001b[1;36m \u001b[0m┃ ┃\u001b[1;36m \u001b[0m\u001b[1;36mColumn Type\u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mCount\u001b[0m\u001b[1;36m \u001b[0m┃                                                          │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n",
       "│ │ Number of rows    │ 2496   │ │ string      │ 1     │                                                          │\n",
       "│ │ Number of columns │ 2      │ │ int64       │ 1     │                                                          │\n",
       "│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n",
       "│ \u001b[3m                                                    number                                                    \u001b[0m  │\n",
       "│ ┏━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┓  │\n",
       "│ ┃\u001b[1m \u001b[0m\u001b[1mcolumn    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA %   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmean      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1msd        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp0  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp25   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp50  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp75  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp100   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mhist    \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┡━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━┩  │\n",
       "│ │ \u001b[38;5;141mlabel     \u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m      0\u001b[0m │ \u001b[36m    0.6707\u001b[0m │ \u001b[36m    0.4701\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m     0\u001b[0m │ \u001b[36m    1\u001b[0m │ \u001b[36m    1\u001b[0m │ \u001b[36m      1\u001b[0m │ \u001b[32m ▃    ▇ \u001b[0m │  │\n",
       "│ └────────────┴──────┴─────────┴────────────┴────────────┴──────┴────────┴───────┴───────┴─────────┴──────────┘  │\n",
       "│ \u001b[3m                                                    string                                                    \u001b[0m  │\n",
       "│ ┏━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓  │\n",
       "│ ┃\u001b[1m        \u001b[0m┃\u001b[1m    \u001b[0m┃\u001b[1m      \u001b[0m┃\u001b[1m            \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m            \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mchars per \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mwords per\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mtotal     \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┃\u001b[1m \u001b[0m\u001b[1mcolumn\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA %\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mshortest  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlongest  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmin       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmax      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrow       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrow      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mwords     \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┡━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩  │\n",
       "│ │ \u001b[38;5;141mtext  \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[38;5;141mI hate    \u001b[0m │ \u001b[38;5;141mWhen will\u001b[0m │ \u001b[38;5;141m\"Muslims  \u001b[0m │ \u001b[38;5;141mYour     \u001b[0m │ \u001b[36m      47.2\u001b[0m │ \u001b[36m      8.8\u001b[0m │ \u001b[36m     22086\u001b[0m │  │\n",
       "│ │        │    │      │ \u001b[38;5;141myou.      \u001b[0m │ \u001b[38;5;141mit       \u001b[0m │ \u001b[38;5;141mare a     \u001b[0m │ \u001b[38;5;141mtrans    \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mfinally  \u001b[0m │ \u001b[38;5;141mcancer\" is\u001b[0m │ \u001b[38;5;141mlivs are \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mbe clear \u001b[0m │ \u001b[38;5;141msuch a    \u001b[0m │ \u001b[38;5;141mworthless\u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mto people\u001b[0m │ \u001b[38;5;141mdehumanisi\u001b[0m │ \u001b[38;5;141m.        \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mthat it's\u001b[0m │ \u001b[38;5;141mng thing  \u001b[0m │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141min a     \u001b[0m │ \u001b[38;5;141mto say.   \u001b[0m │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mdisabled \u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mperson's \u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mnature to\u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mbe an    \u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mannoying \u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mprick?   \u001b[0m │            │           │            │           │            │  │\n",
       "│ └────────┴────┴──────┴────────────┴───────────┴────────────┴───────────┴────────────┴───────────┴────────────┘  │\n",
       "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "label\n",
      "1    1674\n",
      "0     822\n",
      "Name: count, dtype: int64\n",
      "Any missing values in the file: False\n",
      "Number of missing values in the file: 0\n",
      "Number of duplicates in the file: 0\n",
      "Number of missing values in the file after cleaning: 0\n",
      "Number of duplicates in the file after cleaning: 0\n",
      "label\n",
      "1    1674\n",
      "0     822\n",
      "Name: count, dtype: int64\n",
      "Initial maximum sentence length:  24\n",
      "Initial minimum sentence length:  5\n",
      "Maximum sentence length:  27\n",
      "In my opinion , immigrants like you deserve to die .\n",
      "She should be kicked off the platform for saying \" Muslims should all be killed \" .\n",
      "The minority class is: 0\n",
      "Current file opened:  ['data/en_val.csv\\\\data/en_val.csv']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
       "│ <span style=\"font-style: italic\">         Data Summary         </span> <span style=\"font-style: italic\">      Data Types       </span>                                                          │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n",
       "│ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Dataframe         </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Values </span>┃ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Column Type </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Count </span>┃                                                          │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n",
       "│ │ Number of rows    │ 624    │ │ string      │ 1     │                                                          │\n",
       "│ │ Number of columns │ 2      │ │ int64       │ 1     │                                                          │\n",
       "│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n",
       "│ <span style=\"font-style: italic\">                                                    number                                                    </span>  │\n",
       "│ ┏━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┓  │\n",
       "│ ┃<span style=\"font-weight: bold\"> column     </span>┃<span style=\"font-weight: bold\"> NA   </span>┃<span style=\"font-weight: bold\"> NA %    </span>┃<span style=\"font-weight: bold\"> mean       </span>┃<span style=\"font-weight: bold\"> sd         </span>┃<span style=\"font-weight: bold\"> p0   </span>┃<span style=\"font-weight: bold\"> p25    </span>┃<span style=\"font-weight: bold\"> p50   </span>┃<span style=\"font-weight: bold\"> p75   </span>┃<span style=\"font-weight: bold\"> p100    </span>┃<span style=\"font-weight: bold\"> hist     </span>┃  │\n",
       "│ ┡━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━┩  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">label     </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    0.7147</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    0.4519</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">     0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      1</span> │ <span style=\"color: #008000; text-decoration-color: #008000\"> ▃    ▇ </span> │  │\n",
       "│ └────────────┴──────┴─────────┴────────────┴────────────┴──────┴────────┴───────┴───────┴─────────┴──────────┘  │\n",
       "│ <span style=\"font-style: italic\">                                                    string                                                    </span>  │\n",
       "│ ┏━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓  │\n",
       "│ ┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\">    </span>┃<span style=\"font-weight: bold\">      </span>┃<span style=\"font-weight: bold\">            </span>┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\">            </span>┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\"> chars per  </span>┃<span style=\"font-weight: bold\"> words per </span>┃<span style=\"font-weight: bold\"> total      </span>┃  │\n",
       "│ ┃<span style=\"font-weight: bold\"> column </span>┃<span style=\"font-weight: bold\"> NA </span>┃<span style=\"font-weight: bold\"> NA % </span>┃<span style=\"font-weight: bold\"> shortest   </span>┃<span style=\"font-weight: bold\"> longest   </span>┃<span style=\"font-weight: bold\"> min        </span>┃<span style=\"font-weight: bold\"> max       </span>┃<span style=\"font-weight: bold\"> row        </span>┃<span style=\"font-weight: bold\"> row       </span>┃<span style=\"font-weight: bold\"> words      </span>┃  │\n",
       "│ ┡━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">text  </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">I hate    </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">When will</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">A Muslim  </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Your     </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      46.6</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      8.8</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      5499</span> │  │\n",
       "│ │        │    │      │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">pizza.    </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">it       </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">must have </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">obvious  </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">finally  </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">committed </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">contempt </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">be clear </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">that      </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">for      </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">to people</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">crime.    </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Muslims  </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">that it's</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Only they </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">has to   </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">in an    </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">can be    </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">stop     </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">immigrant</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">that      </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">right    </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">'s nature</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">violent   </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">now.     </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">to be an </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">and cruel.</span> │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">annoying </span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">prick?   </span> │            │           │            │           │            │  │\n",
       "│ └────────┴────┴──────┴────────────┴───────────┴────────────┴───────────┴────────────┴───────────┴────────────┘  │\n",
       "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
       "│ \u001b[3m         Data Summary         \u001b[0m \u001b[3m      Data Types       \u001b[0m                                                          │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n",
       "│ ┃\u001b[1;36m \u001b[0m\u001b[1;36mDataframe        \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValues\u001b[0m\u001b[1;36m \u001b[0m┃ ┃\u001b[1;36m \u001b[0m\u001b[1;36mColumn Type\u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mCount\u001b[0m\u001b[1;36m \u001b[0m┃                                                          │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n",
       "│ │ Number of rows    │ 624    │ │ string      │ 1     │                                                          │\n",
       "│ │ Number of columns │ 2      │ │ int64       │ 1     │                                                          │\n",
       "│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n",
       "│ \u001b[3m                                                    number                                                    \u001b[0m  │\n",
       "│ ┏━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┓  │\n",
       "│ ┃\u001b[1m \u001b[0m\u001b[1mcolumn    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA %   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmean      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1msd        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp0  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp25   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp50  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp75  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp100   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mhist    \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┡━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━┩  │\n",
       "│ │ \u001b[38;5;141mlabel     \u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m      0\u001b[0m │ \u001b[36m    0.7147\u001b[0m │ \u001b[36m    0.4519\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m     0\u001b[0m │ \u001b[36m    1\u001b[0m │ \u001b[36m    1\u001b[0m │ \u001b[36m      1\u001b[0m │ \u001b[32m ▃    ▇ \u001b[0m │  │\n",
       "│ └────────────┴──────┴─────────┴────────────┴────────────┴──────┴────────┴───────┴───────┴─────────┴──────────┘  │\n",
       "│ \u001b[3m                                                    string                                                    \u001b[0m  │\n",
       "│ ┏━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓  │\n",
       "│ ┃\u001b[1m        \u001b[0m┃\u001b[1m    \u001b[0m┃\u001b[1m      \u001b[0m┃\u001b[1m            \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m            \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mchars per \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mwords per\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mtotal     \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┃\u001b[1m \u001b[0m\u001b[1mcolumn\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA %\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mshortest  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlongest  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmin       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmax      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrow       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrow      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mwords     \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┡━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩  │\n",
       "│ │ \u001b[38;5;141mtext  \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[38;5;141mI hate    \u001b[0m │ \u001b[38;5;141mWhen will\u001b[0m │ \u001b[38;5;141mA Muslim  \u001b[0m │ \u001b[38;5;141mYour     \u001b[0m │ \u001b[36m      46.6\u001b[0m │ \u001b[36m      8.8\u001b[0m │ \u001b[36m      5499\u001b[0m │  │\n",
       "│ │        │    │      │ \u001b[38;5;141mpizza.    \u001b[0m │ \u001b[38;5;141mit       \u001b[0m │ \u001b[38;5;141mmust have \u001b[0m │ \u001b[38;5;141mobvious  \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mfinally  \u001b[0m │ \u001b[38;5;141mcommitted \u001b[0m │ \u001b[38;5;141mcontempt \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mbe clear \u001b[0m │ \u001b[38;5;141mthat      \u001b[0m │ \u001b[38;5;141mfor      \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mto people\u001b[0m │ \u001b[38;5;141mcrime.    \u001b[0m │ \u001b[38;5;141mMuslims  \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mthat it's\u001b[0m │ \u001b[38;5;141mOnly they \u001b[0m │ \u001b[38;5;141mhas to   \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141min an    \u001b[0m │ \u001b[38;5;141mcan be    \u001b[0m │ \u001b[38;5;141mstop     \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mimmigrant\u001b[0m │ \u001b[38;5;141mthat      \u001b[0m │ \u001b[38;5;141mright    \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141m's nature\u001b[0m │ \u001b[38;5;141mviolent   \u001b[0m │ \u001b[38;5;141mnow.     \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mto be an \u001b[0m │ \u001b[38;5;141mand cruel.\u001b[0m │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mannoying \u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mprick?   \u001b[0m │            │           │            │           │            │  │\n",
       "│ └────────┴────┴──────┴────────────┴───────────┴────────────┴───────────┴────────────┴───────────┴────────────┘  │\n",
       "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "label\n",
      "1    446\n",
      "0    178\n",
      "Name: count, dtype: int64\n",
      "Any missing values in the file: False\n",
      "Number of missing values in the file: 0\n",
      "Number of duplicates in the file: 0\n",
      "Number of missing values in the file after cleaning: 0\n",
      "Number of duplicates in the file after cleaning: 0\n",
      "label\n",
      "1    446\n",
      "0    178\n",
      "Name: count, dtype: int64\n",
      "Initial maximum sentence length:  23\n",
      "Initial minimum sentence length:  5\n",
      "Current file opened:  ['data/en_test.csv\\\\data/en_test.csv']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
       "│ <span style=\"font-style: italic\">         Data Summary         </span> <span style=\"font-style: italic\">      Data Types       </span>                                                          │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n",
       "│ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Dataframe         </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Values </span>┃ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Column Type </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Count </span>┃                                                          │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n",
       "│ │ Number of rows    │ 781    │ │ string      │ 1     │                                                          │\n",
       "│ │ Number of columns │ 2      │ │ int64       │ 1     │                                                          │\n",
       "│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n",
       "│ <span style=\"font-style: italic\">                                                    number                                                    </span>  │\n",
       "│ ┏━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┓  │\n",
       "│ ┃<span style=\"font-weight: bold\"> column     </span>┃<span style=\"font-weight: bold\"> NA   </span>┃<span style=\"font-weight: bold\"> NA %    </span>┃<span style=\"font-weight: bold\"> mean       </span>┃<span style=\"font-weight: bold\"> sd         </span>┃<span style=\"font-weight: bold\"> p0   </span>┃<span style=\"font-weight: bold\"> p25    </span>┃<span style=\"font-weight: bold\"> p50   </span>┃<span style=\"font-weight: bold\"> p75   </span>┃<span style=\"font-weight: bold\"> p100    </span>┃<span style=\"font-weight: bold\"> hist     </span>┃  │\n",
       "│ ┡━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━┩  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">label     </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    0.6901</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    0.4627</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">     0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      1</span> │ <span style=\"color: #008000; text-decoration-color: #008000\"> ▃    ▇ </span> │  │\n",
       "│ └────────────┴──────┴─────────┴────────────┴────────────┴──────┴────────┴───────┴───────┴─────────┴──────────┘  │\n",
       "│ <span style=\"font-style: italic\">                                                    string                                                    </span>  │\n",
       "│ ┏━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓  │\n",
       "│ ┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\">    </span>┃<span style=\"font-weight: bold\">      </span>┃<span style=\"font-weight: bold\">            </span>┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\">            </span>┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\"> chars per  </span>┃<span style=\"font-weight: bold\"> words per </span>┃<span style=\"font-weight: bold\"> total      </span>┃  │\n",
       "│ ┃<span style=\"font-weight: bold\"> column </span>┃<span style=\"font-weight: bold\"> NA </span>┃<span style=\"font-weight: bold\"> NA % </span>┃<span style=\"font-weight: bold\"> shortest   </span>┃<span style=\"font-weight: bold\"> longest   </span>┃<span style=\"font-weight: bold\"> min        </span>┃<span style=\"font-weight: bold\"> max       </span>┃<span style=\"font-weight: bold\"> row        </span>┃<span style=\"font-weight: bold\"> row       </span>┃<span style=\"font-weight: bold\"> words      </span>┃  │\n",
       "│ ┡━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">text  </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">She is    </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">When will</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">A female  </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Your     </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      47.4</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      8.8</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      6847</span> │  │\n",
       "│ │        │    │      │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">vile.     </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">it       </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">life is   </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">trans    </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">finally  </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">not worth </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">lives are</span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">be clear </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">anything. </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">worthless</span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">to people</span> │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">.        </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">that it's</span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">in a     </span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">black    </span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">person's </span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">nature to</span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">be an    </span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">annoying </span> │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">prick?   </span> │            │           │            │           │            │  │\n",
       "│ └────────┴────┴──────┴────────────┴───────────┴────────────┴───────────┴────────────┴───────────┴────────────┘  │\n",
       "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
       "│ \u001b[3m         Data Summary         \u001b[0m \u001b[3m      Data Types       \u001b[0m                                                          │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n",
       "│ ┃\u001b[1;36m \u001b[0m\u001b[1;36mDataframe        \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValues\u001b[0m\u001b[1;36m \u001b[0m┃ ┃\u001b[1;36m \u001b[0m\u001b[1;36mColumn Type\u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mCount\u001b[0m\u001b[1;36m \u001b[0m┃                                                          │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n",
       "│ │ Number of rows    │ 781    │ │ string      │ 1     │                                                          │\n",
       "│ │ Number of columns │ 2      │ │ int64       │ 1     │                                                          │\n",
       "│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n",
       "│ \u001b[3m                                                    number                                                    \u001b[0m  │\n",
       "│ ┏━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┓  │\n",
       "│ ┃\u001b[1m \u001b[0m\u001b[1mcolumn    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA %   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmean      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1msd        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp0  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp25   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp50  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp75  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp100   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mhist    \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┡━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━┩  │\n",
       "│ │ \u001b[38;5;141mlabel     \u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m      0\u001b[0m │ \u001b[36m    0.6901\u001b[0m │ \u001b[36m    0.4627\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m     0\u001b[0m │ \u001b[36m    1\u001b[0m │ \u001b[36m    1\u001b[0m │ \u001b[36m      1\u001b[0m │ \u001b[32m ▃    ▇ \u001b[0m │  │\n",
       "│ └────────────┴──────┴─────────┴────────────┴────────────┴──────┴────────┴───────┴───────┴─────────┴──────────┘  │\n",
       "│ \u001b[3m                                                    string                                                    \u001b[0m  │\n",
       "│ ┏━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓  │\n",
       "│ ┃\u001b[1m        \u001b[0m┃\u001b[1m    \u001b[0m┃\u001b[1m      \u001b[0m┃\u001b[1m            \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m            \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mchars per \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mwords per\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mtotal     \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┃\u001b[1m \u001b[0m\u001b[1mcolumn\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA %\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mshortest  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlongest  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmin       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmax      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrow       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrow      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mwords     \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┡━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩  │\n",
       "│ │ \u001b[38;5;141mtext  \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[38;5;141mShe is    \u001b[0m │ \u001b[38;5;141mWhen will\u001b[0m │ \u001b[38;5;141mA female  \u001b[0m │ \u001b[38;5;141mYour     \u001b[0m │ \u001b[36m      47.4\u001b[0m │ \u001b[36m      8.8\u001b[0m │ \u001b[36m      6847\u001b[0m │  │\n",
       "│ │        │    │      │ \u001b[38;5;141mvile.     \u001b[0m │ \u001b[38;5;141mit       \u001b[0m │ \u001b[38;5;141mlife is   \u001b[0m │ \u001b[38;5;141mtrans    \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mfinally  \u001b[0m │ \u001b[38;5;141mnot worth \u001b[0m │ \u001b[38;5;141mlives are\u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mbe clear \u001b[0m │ \u001b[38;5;141manything. \u001b[0m │ \u001b[38;5;141mworthless\u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mto people\u001b[0m │            │ \u001b[38;5;141m.        \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mthat it's\u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141min a     \u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mblack    \u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mperson's \u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mnature to\u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mbe an    \u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mannoying \u001b[0m │            │           │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141mprick?   \u001b[0m │            │           │            │           │            │  │\n",
       "│ └────────┴────┴──────┴────────────┴───────────┴────────────┴───────────┴────────────┴───────────┴────────────┘  │\n",
       "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Any missing values in the file: False\n",
      "Number of missing values in the file: 0\n",
      "Number of duplicates in the file: 0\n",
      "Initial maximum sentence length:  24\n",
      "Initial minimum sentence length:  5\n",
      "Maximum sentence length:  26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4301c090265644078a07d7fcabf43081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=1):   0%|          | 0/2496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['index', 'label', 'input_ids', 'attention_mask']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90565efac51f4e5183e81460810eb406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=1):   0%|          | 0/624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599d211143eb481fadd517c1bf26e97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=1):   0%|          | 0/781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ad396ae63643fc988f235359300d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3901 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 27\n",
      "{1: np.float64(1.4910394265232976), 0: np.float64(3.0364963503649633)}\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "##==================== UTILS ====================##\n",
    "\n",
    "# A function that sets seed for reproducibility\n",
    "def set_seed(seed_value):\n",
    "  random.seed(seed_value)\n",
    "  np.random.seed(seed_value)\n",
    "  torch.manual_seed(seed_value)\n",
    "  torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "\n",
    "# A function that checks if a directory exists else creates the directory\n",
    "def check_create_path(path):\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    print('Directory created at {}'.format(path))\n",
    "  else:\n",
    "    print('Directory {} already exists!'.format(path))\n",
    "\n",
    "\n",
    "# A function that reads a csv or tsv file\n",
    "def read_a_csv_tsv_file(filename):\n",
    "  # Check first whether a certain file or directory exists\n",
    "  if os.path.exists(filename):\n",
    "    print('Current file opened: ',[os.path.join(filename, file) for file in glob.glob(filename)])\n",
    "\n",
    "    # Find the file extension to open it properly\n",
    "    find_separator = {'.csv': ',', '.tsv': '\\t'}\n",
    "    basename, format = os.path.splitext(filename)\n",
    "    assert format in find_separator\n",
    "    separator = find_separator[format]\n",
    "\n",
    "    # Read different extensions of files using pandas with 2 different separators\n",
    "    read_file = pd.read_csv(filename, sep = separator, encoding = 'utf-8')\n",
    "\n",
    "    return read_file\n",
    "\n",
    "  else:\n",
    "    print('File or directory not accessible. Please check the filename and ensure that the entered path of the file is in \"tsv\" or \"csv\" form.')\n",
    "\n",
    "\n",
    "def open_json_dataset(json_file, type_split = 1):\n",
    "  # Open the json dataset\n",
    "  if type_split == 1:\n",
    "    dataset = load_dataset('json', data_files = json_file, split = 'train')\n",
    "  elif type_split == 2:\n",
    "    dataset = load_dataset('json', data_files = json_file, split = 'validation')\n",
    "  elif type_split == 3:\n",
    "    dataset = load_dataset('json', data_files = json_file, split = 'test')\n",
    "  else:\n",
    "    print('Please specify the number \"1\" for train set, \"2\" for validation and \"3\" to use the test set.')\n",
    "  print(dataset)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "# A function that opens and reads a dataset either from Hugging Face or from a local directory\n",
    "def open_dataset(dataset_path, text_column, label_column, huggingface_dataset = True, json_dataset = False, csv_tsv_dataset = False, type_split = 1, clean_text = True, labelled_dataset = True):\n",
    "  \"\"\"Opens a dataframe object or Hugging Face dataset, or json dataset converts it into dataframe and presents an overview of values\"\"\"\n",
    "  if huggingface_dataset:\n",
    "    if type_split == 1:\n",
    "      dataset = load_dataset(dataset_path, split='train')\n",
    "      read_file = pd.DataFrame(dataset)\n",
    "    elif type_split == 2:\n",
    "      dataset = load_dataset(dataset_path, split='validation')\n",
    "      read_file = pd.DataFrame(dataset)\n",
    "    elif type_split == 3:\n",
    "      dataset = load_dataset(dataset_path, split='test')\n",
    "      read_file = pd.DataFrame(dataset)\n",
    "  elif json_dataset:\n",
    "    read_file = pd.read_json(dataset_path)\n",
    "  elif csv_tsv_dataset:\n",
    "    read_file = read_a_csv_tsv_file(dataset_path)\n",
    "  else:\n",
    "    print('Please specify whether it is a Hugging Face Dataset, a json dataset or a csv/tsv dataset. For the Hugging Face dataset, select type_split = \"1\" for train set, \"2\" for validation and \"3\" for the test set.')\n",
    "\n",
    "  skimpy_file = skim(read_file)\n",
    "  print(skimpy_file)\n",
    "\n",
    "  if text_column != 'text':\n",
    "    read_file = read_file.rename({text_column:'text'}, axis = 1)\n",
    "  else:\n",
    "    read_file\n",
    "  \n",
    "  if label_column is not None:\n",
    "    if label_column != 'label':\n",
    "      read_file = read_file.rename({label_column:'label'}, axis = 1)\n",
    "    else:\n",
    "      read_file\n",
    "    print(read_file.label.value_counts())\n",
    "  else:\n",
    "    read_file\n",
    "\n",
    "  print('Any missing values in the file:', read_file.isnull().values.any())\n",
    "  print('Number of missing values in the file:', read_file.isnull().sum().sum())\n",
    "  print('Number of duplicates in the file:', read_file.duplicated(subset = 'text').sum())\n",
    "  \n",
    "  if clean_text: \n",
    "    read_file.dropna(axis=1,how='all', inplace = True)\n",
    "    print('Number of missing values in the file after cleaning:', read_file.isnull().sum().sum()) \n",
    "    read_file.drop_duplicates(subset = ['text'], keep = 'first', inplace = True) #, \n",
    "    print('Number of duplicates in the file after cleaning:', read_file.duplicated(subset = 'text').sum())\n",
    "    read_file.reset_index(inplace = True) \n",
    "  else:\n",
    "    read_file\n",
    "  \n",
    "  if label_column is not None:\n",
    "    print(read_file.label.value_counts())\n",
    "  else:\n",
    "    read_file\n",
    "  \n",
    "\n",
    "  if labelled_dataset:\n",
    "    # Encode the concatenated data\n",
    "    encoded_texts = [tokenizer.encode(sent, add_special_tokens = True) for sent in read_file.text.values]\n",
    "    # Find the maximum length\n",
    "    max_len = max([len(sent) for sent in encoded_texts])\n",
    "    print('Initial maximum sentence length: ', max_len)\n",
    "    # Find the minimum length\n",
    "    min_len = min([len(sent) for sent in encoded_texts])\n",
    "    print('Initial minimum sentence length: ', min_len)\n",
    "  else:\n",
    "    None\n",
    "  return read_file\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "  \"\"\"Computes micro-F1 score, macro-F1 score, accuracy on a batch of predictions\"\"\"\n",
    "  logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "  preds = np.argmax(logits, axis=1)\n",
    "  macro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='macro', zero_division=0)\n",
    "  micro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='micro', zero_division=0)\n",
    "  accuracy = accuracy_score(y_true=p.label_ids, y_pred=preds)\n",
    "  return {'micro_f1': micro_f1, 'macro_f1': macro_f1, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "# A function that calculates all the metrics using the validation/test set\n",
    "def calculate_metrics(y_true, preds, class_names, save_directory_name):\n",
    "  print('\\nCALCULATING METRICS...')\n",
    "  \n",
    "  #assert len(preds) == len(y_true)\n",
    "  # Calculate the accuracy of the model\n",
    "  acc = accuracy_score(y_true, preds)\n",
    "  # Calculate the Matthews Correlation Coefficient\n",
    "  # -1 indicates total disagreement between predicted classes and actual classes\n",
    "  # 0 is synonymous with completely random guessing, 1 indicates total agreement between predicted classes and actual classes\n",
    "  mcc = matthews_corrcoef(y_true, preds)\n",
    "  model_f1_score_micro = f1_score(y_true, preds, average = 'micro', zero_division = 1)\n",
    "  model_precision_micro = precision_score(y_true, preds, average = 'micro', zero_division = 1)\n",
    "  model_recall_micro = recall_score(y_true, preds, average = 'micro', zero_division = 1)\n",
    "  model_f1_score_macro = f1_score(y_true, preds, average = 'macro', zero_division = 1)\n",
    "  model_precision_macro = precision_score(y_true, preds, average = 'macro', zero_division = 1)\n",
    "  model_recall_macro = recall_score(y_true, preds, average = 'macro', zero_division = 1)\n",
    "  precision, recall, fscore, support = score(y_true, preds, zero_division = 1)\n",
    "  print(f'Accuracy: {acc}')\n",
    "  print(f'Micro-F1 Score: {model_f1_score_micro}')\n",
    "  print(f'Macro-F1 Score: {model_f1_score_macro}') \n",
    "  print(f'Macro-Precision Score: {model_precision_macro}')\n",
    "  print(f'Macro-Recall Score: {model_recall_macro}')\n",
    "  print(f'Matthews Correlation Coefficient: {mcc}')\n",
    "  print(f'\\nPrecision of each class: {precision}')\n",
    "  print(f'Recall of each class: {recall}')\n",
    "  print(f'F1 score of each class: {fscore}')\n",
    "  print(classification_report(y_true, preds, target_names = class_names, digits=4))\n",
    "  # Create the confusion matrix\n",
    "  cm = confusion_matrix(y_true, preds)\n",
    "  df_cm = pd.DataFrame(cm, index = class_names, columns = class_names)\n",
    "  hmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True Labels')\n",
    "  plt.xlabel('Predicted Labels')\n",
    "  plt.savefig(save_directory_name, bbox_inches='tight')\n",
    "  #plt.show()\n",
    "  plt.close()\n",
    "  return model_f1_score_macro, model_f1_score_micro, fscore, acc, precision, recall, support\n",
    "\n",
    "\n",
    "def tokenize(batch, tokenizer=None, args=None):\n",
    "  return tokenizer(batch['text'], \n",
    "                   max_length = args['max_seq_length'], \n",
    "                   padding='max_length',\n",
    "                   truncation=True,\n",
    "                   add_special_tokens=True,\n",
    "                   return_attention_mask=True,\n",
    "                   return_tensors=\"pt\") \n",
    "\n",
    "# PRE-PROCESSING\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize = ['user', 'url', 'email'],\n",
    "\n",
    "    # terms that will be annotated\n",
    "    #annotate = {'hashtag'},  #{'allcaps', 'repeated', 'elongated'},\n",
    "\n",
    "    # corpus from which the word statistics are going to be used for word segmentation\n",
    "    segmenter = 'twitter',  # or 'english'\n",
    "\n",
    "    # corpus from which the word statistics are going to be used for spell correction\n",
    "    corrector = 'twitter',  # or 'english'\n",
    "\n",
    "    fix_html = False,              # fix HTML tokens\n",
    "    fix_text = False,              # fix text\n",
    "    unpack_hashtags = True,       # perform word segmentation on hashtags\n",
    "    unpack_contractions = False,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong = False,   # spell correction for elongated words\n",
    "\n",
    "    tokenizer = SocialTokenizer(lowercase = False).tokenize)\n",
    "\n",
    "\n",
    "ws.load()\n",
    "def segment_hashtags(text):\n",
    "  text = re.sub('#[\\S]+', lambda match: ' '.join(ws.segment(match.group())), text)\n",
    "  return text\n",
    "\n",
    "def emojis_into_text(sentence):\n",
    "  demojized_sent = emoji.demojize(sentence)\n",
    "  emoji_txt = re.sub(r':[\\S]+:', lambda x: x.group().replace('_', ' ').replace('-', ' ').replace(':', ''), demojized_sent)\n",
    "  return emoji_txt\n",
    "\n",
    "def preprocessing(text):\n",
    "  # Convert the emojis into their textual representation\n",
    "  text = emojis_into_text(text)\n",
    "\n",
    "  # # Replace '&amp;' with 'and'\n",
    "  text = re.sub(r'&amp;','and', text)\n",
    "  text = re.sub(r'&','and', text)\n",
    "\n",
    "  # # # Replace the unicode apostrophe\n",
    "  text = re.sub(r\"\\?\",\"'\", text)\n",
    "  text = re.sub(r'\\?','\"', text)\n",
    " \n",
    "  # Replace consecutive non-ASCII characters with whitespace\n",
    "  text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "\n",
    "  text = re.sub(' +',' ', text) \n",
    "\n",
    "  # Apply the text processor from ekphrasis library\n",
    "  text = ' '.join(text_processor.pre_process_doc(text))\n",
    "\n",
    "  # Apply hashtag segmentation\n",
    "  text = segment_hashtags(text)\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "# A function that splits the data into training and validation\n",
    "def data_splitting(dataframe, text_column, label_column, split_ratio):\n",
    "  x_train_texts, y_val_texts, x_train_labels, y_val_labels = train_test_split(dataframe[text_column], dataframe[label_column],\n",
    "                                                                              random_state = 42,\n",
    "                                                                              test_size = split_ratio,\n",
    "                                                                              stratify = dataframe[label_column])\n",
    "  print(f'Dataset split into train and validation/test sets using {split_ratio} split ratio.')\n",
    "  train_df = pd.concat([x_train_texts, x_train_labels], axis = 1)\n",
    "  val_df = pd.concat([y_val_texts, y_val_labels], axis = 1)\n",
    "  print(f'Size of training set: {len(train_df)}')\n",
    "  print(f'Size of validation/test set: {len(val_df)}')\n",
    "  return train_df, val_df\n",
    "\n",
    "\n",
    "def compute_class_weights(classes):\n",
    "  weight1, weight2 = compute_class_weight(class_weight = 'balanced',\n",
    "                                      classes = np.unique(classes),\n",
    "                                      y = classes)\n",
    "  print(f'Weight for class 0: {weight1}')\n",
    "  print(f'Weight for class 1: {weight2}')\n",
    "  return weight1, weight2\n",
    "\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "  def __init__(self, trainer) -> None:\n",
    "    super().__init__()\n",
    "    self._trainer = trainer\n",
    "\n",
    "  def on_epoch_end(self, args, state, control, **kwargs):\n",
    "    if control.should_evaluate:\n",
    "      control_copy = deepcopy(control)\n",
    "      self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\") #####\n",
    "      return control_copy\n",
    "    \n",
    "  \n",
    "##==================== PARAMETERS & TOKENIZER INITIALIZATION ====================##\n",
    "dict_text_classification_model_names = {1: 'bert-large-uncased',\n",
    "                                        2: 'bert-base-multilingual-uncased',\n",
    "                                        3: 'albert-xlarge-v1',\n",
    "                                        4: 'albert-xlarge-v2',\n",
    "                                        5: 'albert-xxlarge-v1',\n",
    "                                        6: 'albert-xxlarge-v2',\n",
    "                                        7: 'roberta-large',\n",
    "                                        8: 'xlm-roberta-large',\n",
    "                                        9: 'microsoft/deberta-large',\n",
    "                                        10: 'microsoft/deberta-xlarge',\n",
    "                                        11 : 'microsoft/deberta-v2-xlarge',\n",
    "                                        12 : 'microsoft/deberta-v2-xxlarge',\n",
    "                                        13 : 'microsoft/deberta-v3-large',\n",
    "                                        14 : 'microsoft/mdeberta-v3-base',\n",
    "                                        15 : 'GroNLP/hateBERT',\n",
    "                                        16: 'vinai/bertweet-base',\n",
    "                                        17: 'cardiffnlp/twitter-xlm-roberta-base'}\n",
    "\n",
    "dict_causal_model_names = {1: 'bigscience/bloomz-560m',\n",
    "                           2: 'NousResearch/Llama-2-7b-chat-hf',\n",
    "                           3: 'NousResearch/Llama-2-13b-chat-hf',\n",
    "                           4: 'microsoft/DialoGPT-medium',\n",
    "                           5: 'microsoft/DialoGPT-large',\n",
    "                           6: 'Open-Orca/Mistral-7B-OpenOrca', \n",
    "                           7: 'HuggingFaceH4/zephyr-7b-alpha', \n",
    "                           8: 'mistralai/Mistral-7B-v0.1', \n",
    "                           9: 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "                           10: 'HuggingFaceH4/zephyr-7b-beta', \n",
    "                           11: 'NousResearch/Llama-2-7b-hf',\n",
    "                           12: 'NousResearch/Llama-2-13b-hf',\n",
    "                           13: 'TheBloke/Mistral-7B-OpenOrca-GPTQ',\n",
    "                           14: 'TheBloke/Llama-2-7b-Chat-GPTQ',\n",
    "                           15: 'TheBloke/zephyr-7B-beta-GPTQ',\n",
    "                           16: 'TheBloke/Llama-2-7B-GPTQ',\n",
    "                           17: 'Mistral-7B-v0.1',\n",
    "                           18: 'decapoda-research/llama-7b-hf',\n",
    "                           19: 'mistralai/Mixtral-8x7B-v0.1',\n",
    "                           20: 'meta-llama/Llama-3.2-1B',\n",
    "                           21: 'meta-llama/Llama-3.2-3B'}\n",
    "\n",
    "dict_seq_seq_model_names = {1: 't5-large',\n",
    "                            2: 'microsoft/GODEL-v1_1-large-seq2seq',\n",
    "                            3: 'bigscience/mt0-large'}\n",
    "\n",
    "dict_model_type = {1: 'TEXT_CLASSIF',\n",
    "                   2: 'CAUSAL',\n",
    "                   3: 'SEQ2SEQ',\n",
    "                   4: 'TOKEN_CLASSIF',\n",
    "                   5: 'IMAGE_CLASSIF'}\n",
    "\n",
    "dict_task_type = {1: 'SEQ_CLS',\n",
    "                  2: 'CAUSAL_LM',\n",
    "                  3: 'SEQ_2_SEQ_LM',\n",
    "                  4: 'TOKEN_CLS'}\n",
    "\n",
    "dict_config_type = {1: 'PrefixTuning',\n",
    "                    2: 'PromptTuning',\n",
    "                    3: 'LoRA',\n",
    "                    4: 'PromptEncoder'}\n",
    "\n",
    "args = {'task_name': '=== teste bloomz en ===',\n",
    "        'data_directory': '/data/',\n",
    "        'results_data_directory': '/data/results/',\n",
    "        'output_model_directory': '/data/outputs/',\n",
    "        'model_name': str(dict_causal_model_names[20]), # Change the dictionary name and the index of the model of your choice\n",
    "        'model_type': str(dict_model_type[1]), # Change the index for text classification/causal language modeling/Sequence2Sequence\n",
    "        'task_type' : str(dict_task_type[2]), # Change the index for text classification/causal language modeling/Sequence2Sequence\n",
    "        'new_model_name': 'bloomz_test',\n",
    "        'config': str(dict_config_type[2]), \n",
    "        'inference_mode': True,\n",
    "        'num_virtual_tokens': 37, \n",
    "        'modules_to_save': ['classifier'], \n",
    "        'num_classes': 2,\n",
    "        'max_seq_length': 195, \n",
    "        'data_split_ratio': 0.2,\n",
    "        'train_batch_size': 4,#16, \n",
    "        'validation_batch_size': 4,#16,\n",
    "        'num_train_epochs': 2,#10, \n",
    "        'warmup_steps': 0,\n",
    "        'weight_decay':  0.0001, \n",
    "        'learning_rate': 1e-4, \n",
    "        'adam_epsilon': 1e-8,\n",
    "        'gradient_accumulation_steps': 2,\n",
    "        'gradient_checkpointing': True,\n",
    "        'max_grad_norm': 0.3, \n",
    "        'early_stopping_patience': 5,\n",
    "        'seed': 42,\n",
    "        'optimizer':'paged_adamw_32bit', \n",
    "        'lr_scheduler_type': 'constant', \n",
    "        'warmup_ratio': 0.1,\n",
    "        'group_by_length': True,              # Group sequences into batches with same length. Saves memory and speeds up training considerably\n",
    "        'save_steps': 1000,                   # Save checkpoint every X updates steps\n",
    "        'logging_steps': 1000,\n",
    "        'evaluation_strategy': 'epoch',\n",
    "        'save_strategy':'epoch',\n",
    "        'eval_steps': 1000,\n",
    "        'save_total_limit': 2,\n",
    "        'packing': False,                     # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "        'fp16': False,\n",
    "        'bf16': False,\n",
    "        'greater_is_better': True,\n",
    "        'load_best_model_at_end': True,\n",
    "        'overwrite_output_dir': True,\n",
    "        'push_to_hub': False,\n",
    "        'report_to': 'none', \n",
    "        'hub_strategy': 'every_save',\n",
    "        'ignore_pad_token_for_loss': True,\n",
    "        'problem_type': 'single_label_classification',\n",
    "        'prompt_tuning_init_text': \"[INST]Your task is to classify if the text contains hate speech or not, and return the answer as the corresponding label '0' or '1'[/INST]\",\n",
    "\n",
    "        # bitsandbytes parameters\n",
    "        'use_4bit': True,                     # Activate 4-bit precision base model loading\n",
    "        'bnb_4bit_compute_dtype': 'float16',  # Compute dtype for 4-bit base models\n",
    "        'bnb_4bit_quant_type': 'nf4',         # Quantization type (fp4 or nf4)\n",
    "        'use_nested_quant': False,            # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "        # QLora Parameters\n",
    "        'lora_r': 16,                          # LoRA attention dimension\n",
    "        'lora_alpha': 16,                     # Alpha parameter for LoRA scaling\n",
    "        'lora_dropout': 0.05,                 # Dropout probability for LoRA layers \n",
    "        }\n",
    "\n",
    "print('================',str(args['task_name']),'================\\n')\n",
    "\n",
    "# Get the directory names and the specific model used\n",
    "print('Output directory: ' + str(args['output_model_directory']))\n",
    "print('Model Name: ' + str(args['model_name']))\n",
    "args['output_specific_model_dir'] = args['output_model_directory'] + args['model_name'] + '/' \n",
    "print('Output Directory: ' + str(args['output_specific_model_dir']))\n",
    "\n",
    "# Check whether the directories exist else create them\n",
    "print('\\nChecking that the necessary paths exist...')\n",
    "check_create_path(args['data_directory'])\n",
    "check_create_path(args['output_model_directory'])\n",
    "check_create_path(args['results_data_directory'])\n",
    "check_create_path(args['output_specific_model_dir'])\n",
    "\n",
    "repository_id = args['new_model_name'] \n",
    "\n",
    "MODEL_CLASSES = {'TEXT_CLASSIF': (AutoConfig, AutoModelForSequenceClassification, AutoTokenizer),\n",
    "                 'CAUSAL' : (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n",
    "                 'SEQ2SEQ': (AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer),\n",
    "                 'TOKEN_CLASSIF' : (AutoConfig, AutoModelForTokenClassification, AutoTokenizer),\n",
    "                 'IMAGE_CLASSIF' : (AutoConfig, AutoModelForImageClassification, AutoTokenizer)}\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, args['bnb_4bit_compute_dtype'])\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = args['use_4bit'],\n",
    "    bnb_4bit_quant_type = args['bnb_4bit_quant_type'],\n",
    "    bnb_4bit_compute_dtype = args['bnb_4bit_compute_dtype'],\n",
    "    bnb_4bit_use_double_quant = args['use_nested_quant'])\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and args['use_4bit']:\n",
    "  major, _ = torch.cuda.get_device_capability()\n",
    "  if major >= 8:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(args['seed'])\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(args['model_name'], add_prefix_space=False,\n",
    "                                            use_fast = True, trust_remote_code=True, add_eos_token=True, add_bos_token = True, padding_side = 'left',\n",
    "                                            do_lower_case = False)  \n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_tokens(['<user>', '<url>', '<email>'], special_tokens = True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer, padding = 'longest')\n",
    "\n",
    "tokens = tokenizer.tokenize(args['prompt_tuning_init_text'])\n",
    "num_tokens = len(tokens)\n",
    "print(f\"The prompt contains {num_tokens} tokens.\")\n",
    "\n",
    "##==================== LABELS OF SUB-TASKS ====================##\n",
    "label2id = {'NON-HATE': 0, 'HATE': 1}\n",
    "id2label = {0: 'NON-HATE', 1: 'HATE'}\n",
    "class_names = ['NON-HATE', 'HATE']\n",
    "\n",
    "##==================== OPEN AND PROCESS DATASETS ====================##\n",
    "training_data = open_dataset('data/en_train.csv',\n",
    "                            #args['data_directory'] + 'en_train.csv', \n",
    "                            'text', 'label', huggingface_dataset = False, json_dataset = False, csv_tsv_dataset = True, \n",
    "                             type_split = 1, clean_text= True, labelled_dataset = True)\n",
    "\n",
    "training_data['text'] = training_data['text'].apply(lambda x: preprocessing(x))\n",
    "\n",
    "# Encode the concatenated data\n",
    "encoded_texts = [tokenizer.encode(sent, add_special_tokens = True) for sent in training_data['text'].values]\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_texts])\n",
    "print('Maximum sentence length: ', max_len)\n",
    "\n",
    "print(training_data['text'][0])\n",
    "print(training_data['text'][8])\n",
    "\n",
    "counts = training_data['label'].value_counts()\n",
    "minority_class = counts.idxmin()\n",
    "print(f'The minority class is: {minority_class}') \n",
    "\n",
    "# df1 = pd.read_csv(args['data_directory'] + 'SubTask-A-(index,tweet)val.csv')\n",
    "# df2 = pd.read_csv(args['data_directory'] + 'SubTask-A(index,label)val.csv')\n",
    "# data = pd.merge(df1, df2, on='index')\n",
    "# # print(data)\n",
    "# data.to_csv(args['data_directory'] + 'SubTask-A_labelled_val.csv', index=False)\n",
    "\n",
    "val_data = open_dataset('data/en_val.csv', \n",
    "                                  'text', 'label', huggingface_dataset = False, json_dataset = False, csv_tsv_dataset = True, \n",
    "                                   type_split = 1, clean_text= True, labelled_dataset = True)\n",
    "\n",
    "val_data['text'] = val_data['text'].apply(lambda x: preprocessing(x))\n",
    "\n",
    "# df1 = pd.read_csv(args['data_directory'] + 'SubTask-A-(index,tweet)test.csv')\n",
    "# df2 = pd.read_csv(args['data_directory'] + 'SubTask-A(index,label)test.csv')\n",
    "# data = pd.merge(df1, df2, on='index') \n",
    "# data.to_csv(args['data_directory'] + 'SubTask-A_labelled_test.csv', index=False)\n",
    "\n",
    "test_data = open_dataset('data/en_test.csv',\n",
    "                                  'text', None, huggingface_dataset = False, json_dataset = False, csv_tsv_dataset = True, \n",
    "                                   type_split = 1, clean_text= False, labelled_dataset = True)\n",
    "\n",
    "test_data['text'] = test_data['text'].apply(lambda x: preprocessing(x))\n",
    "\n",
    "# Encode the concatenated data\n",
    "encoded_texts_test = [tokenizer.encode(sent, add_special_tokens = True) for sent in test_data['text'].values]\n",
    "# Find the maximum length\n",
    "max_len_test = max([len(sent) for sent in encoded_texts_test])\n",
    "print('Maximum sentence length: ', max_len_test)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(training_data, split='train')\n",
    "encoded_train_dataset = train_dataset.map(\n",
    "    tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"args\": args},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns = ['text'],\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\")\n",
    "encoded_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "print(f\"Keys of tokenized dataset: {list(encoded_train_dataset.features)}\")\n",
    "\n",
    "validation_dataset = Dataset.from_pandas(val_data, split='validation')\n",
    "encoded_val_dataset = validation_dataset.map(tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"args\": args},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns = ['text'],\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\")\n",
    "encoded_val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label']) \n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_data, split='test')\n",
    "encoded_test_dataset = test_dataset.map(\n",
    "    tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"args\": args},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns = ['text'],\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\")   \n",
    "encoded_test_dataset.set_format('torch', columns=['input_ids', 'attention_mask']) \n",
    "\n",
    "tokenized_inputs = concatenate_datasets([train_dataset, validation_dataset, test_dataset]).map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=['text', 'label'])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# Calculate the total number of samples\n",
    "class_counts = training_data['label'].value_counts()\n",
    "total_samples = sum(class_counts.values)\n",
    "# Calculate the weights\n",
    "weights = {class_id: total_samples / num_samples_in_class for class_id, num_samples_in_class in class_counts.items()}\n",
    "print(weights) \n",
    "\n",
    "\n",
    "def get_weighted_trainer(classes):\n",
    "  # You can use this weights if you want to balance the classes\n",
    "  #weights_class_1, weight_class_2 = compute_class_weights(classes)\n",
    "    \n",
    "  class _WeightedBCELossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "      labels = inputs.pop(\"labels\")     \n",
    "      outputs = model(**inputs)\n",
    "      logits = outputs.get(\"logits\")\n",
    "      loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([weights[0], weights[1]], device=labels.device, dtype=logits.dtype)) \n",
    "      loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "      return (loss, outputs) if return_outputs else loss\n",
    "  return _WeightedBCELossTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7103d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-18 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc6 in position 8: invalid continuation byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 75,776 || all params: 1,235,896,320 || trainable%: 0.0061\n"
     ]
    }
   ],
   "source": [
    "##==================== MODEL INITIALIZATION ====================##\n",
    "model = model_class.from_pretrained(args['model_name'], \n",
    "                                    num_labels = args['num_classes'], \n",
    "                                    id2label = id2label,\n",
    "                                    label2id = label2id,\n",
    "                                    device_map = 'auto', \n",
    "                                    offload_folder = 'offload',\n",
    "                                    trust_remote_code = True, \n",
    "                                    torch_dtype = torch.float16,\n",
    "                                    quantization_config = bnb_config)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "if args['config'] == 'PrefixTuning':\n",
    "  '''\n",
    "  Prefix tuning is an additive method where only a sequence of continuous task-specific vectors is attached to the beginning of the input, or prefix. \n",
    "  Only the prefix parameters are optimized and added to the hidden states in every layer of the model. The tokens of the input sequence can still attend to the prefix as virtual tokens. \n",
    "  As a result, prefix tuning stores 1000x fewer parameters than a fully finetuned model, which means you can use one LLM for many tasks.\n",
    "  '''\n",
    "  peft_config = PrefixTuningConfig(task_type = args['task_type'], inference_mode = args['inference_mode'], num_virtual_tokens = args['num_virtual_tokens'])\n",
    "\n",
    "elif args['config'] == 'PromptTuning':\n",
    "  '''\n",
    "  Prompting helps guide language model behavior by adding some input text specific to a task. Prompt tuning is an additive method for only training and updating the newly added prompt tokens to a pretrained model. \n",
    "  This way, you can use one pretrained model whose weights are frozen, and train and update a smaller set of prompt parameters for each downstream task instead of fully finetuning a separate model. \n",
    "  As models grow larger and larger, prompt tuning can be more efficient, and results are even better as model parameters scale.\n",
    "  '''\n",
    "  peft_config = PromptTuningConfig(task_type = args['task_type'],\n",
    "                                    prompt_tuning_init = PromptTuningInit.TEXT,\n",
    "                                    num_virtual_tokens = args['num_virtual_tokens'],\n",
    "                                    prompt_tuning_init_text= args['prompt_tuning_init_text'],\n",
    "                                    tokenizer_name_or_path = args['model_name'])\n",
    "elif args['config'] == 'LoRA':\n",
    "  '''\n",
    "  LoRA, a technique that accelerates the fine-tuning of large models while consuming less memory. To make fine-tuning more efficient, LoRA's approach is to represent the weight updates with two smaller matrices (called update matrices) through low-rank decomposition. \n",
    "  These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn't receive any further adjustments. \n",
    "  To produce the final results, both the original and the adapted weights are combined.\n",
    "  '''\n",
    "  peft_config = LoraConfig(lora_alpha = args['lora_alpha'], \n",
    "                           lora_dropout = args['lora_dropout'], \n",
    "                           r = args['lora_r'], \n",
    "                           bias = 'none',\n",
    "                           target_modules = ['q_proj','v_proj'],\n",
    "                           task_type = args['task_type'])\n",
    "\n",
    "# P-tuning is a method for automatically searching and optimizing for better prompts in a continuous space \n",
    "elif args['config'] == 'PromptEncoder':\n",
    "  peft_config = PromptEncoderConfig(task_type = args['task_type'], num_virtual_tokens = args['num_virtual_tokens'], encoder_hidden_size=128)\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf0678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e171f4fc3af412fbb1dc278bb31433f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d5ea49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##==================== RUN EXPERIMENTS ====================##\n",
    "\n",
    "\n",
    "# Set training parameters\n",
    "arguments = TrainingArguments(\n",
    "    output_dir = repository_id,\n",
    "    logging_dir = f'{repository_id}/logs',\n",
    "    eval_strategy = args['evaluation_strategy'],\n",
    "    save_strategy = args['save_strategy'],\n",
    "    eval_steps = args['eval_steps'],\n",
    "    save_total_limit = args['save_total_limit'],\n",
    "    learning_rate = args['learning_rate'],\n",
    "    num_train_epochs = args['num_train_epochs'],\n",
    "    metric_for_best_model = 'macro_f1',\n",
    "    greater_is_better = args['greater_is_better'],\n",
    "    weight_decay = args['weight_decay'],\n",
    "    load_best_model_at_end = args['load_best_model_at_end'],\n",
    "    per_device_train_batch_size = args['train_batch_size'],\n",
    "    per_device_eval_batch_size = args['validation_batch_size'],\n",
    "    overwrite_output_dir = args['overwrite_output_dir'],\n",
    "    fp16 = args['fp16'],\n",
    "    bf16 = args['bf16'],\n",
    "    seed = args['seed'],\n",
    "    warmup_ratio = args['warmup_steps'],\n",
    "    gradient_accumulation_steps = args['gradient_accumulation_steps'],\n",
    "    gradient_checkpointing = args['gradient_checkpointing'],\n",
    "    optim = args['optimizer'],\n",
    "    save_steps = args['save_steps'],\n",
    "    logging_strategy = 'steps',\n",
    "    logging_steps = args['logging_steps'],\n",
    "    max_grad_norm = args['max_grad_norm'],\n",
    "    group_by_length = args['group_by_length'],\n",
    "    lr_scheduler_type = args['lr_scheduler_type'],\n",
    "    report_to = args['report_to'],\n",
    "    push_to_hub = args['push_to_hub'],\n",
    "    hub_strategy = args['hub_strategy'])\n",
    "\n",
    "weighted_trainer = get_weighted_trainer(training_data['label'])\n",
    "\n",
    "trainer = weighted_trainer(  \n",
    "     model = model,\n",
    "     data_collator = data_collator,\n",
    "     tokenizer = tokenizer,\n",
    "     args = arguments,\n",
    "     train_dataset = encoded_train_dataset,        \n",
    "     eval_dataset = encoded_val_dataset,        \n",
    "     compute_metrics = compute_metrics,\n",
    "     callbacks = [EarlyStoppingCallback(early_stopping_patience = args['early_stopping_patience'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e6e0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (59512176) to match target batch_size (4).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAINING...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39madd_callback(CustomCallback(trainer)) \n\u001b[1;32m----> 4\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\transformers\\trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   4026\u001b[0m ):\n",
      "Cell \u001b[1;32mIn[2], line 574\u001b[0m, in \u001b[0;36mget_weighted_trainer.<locals>._WeightedBCELossTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m    572\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    573\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([weights[\u001b[38;5;241m0\u001b[39m], weights[\u001b[38;5;241m1\u001b[39m]], device\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdtype)) \n\u001b[1;32m--> 574\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1385\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\torch\\nn\\functional.py:3458\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3457\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3459\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3462\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3465\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (59512176) to match target batch_size (4)."
     ]
    }
   ],
   "source": [
    "##==================== TRAIN & EVALUATE ====================##\n",
    "print('TRAINING...')\n",
    "trainer.add_callback(CustomCallback(trainer)) \n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073f102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.5402\n",
      "  train_runtime            = 0:10:40.41\n",
      "  train_samples            =       2496\n",
      "  train_samples_per_second =      7.795\n",
      "  train_steps_per_second   =      0.974\n",
      "EVALUATING...\n",
      "{'eval_loss': 0.285400390625, 'eval_micro_f1': 0.8878205128205128, 'eval_macro_f1': 0.8705363494095888, 'eval_accuracy': 0.8878205128205128, 'eval_runtime': 18.4053, 'eval_samples_per_second': 33.903, 'eval_steps_per_second': 8.476, 'epoch': 2.0}\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.8878\n",
      "  eval_loss               =     0.2854\n",
      "  eval_macro_f1           =     0.8705\n",
      "  eval_micro_f1           =     0.8878\n",
      "  eval_runtime            = 0:00:18.40\n",
      "  eval_samples            =        624\n",
      "  eval_samples_per_second =     33.903\n",
      "  eval_steps_per_second   =      8.476\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained(repository_id)\n",
    "#trainer.create_model_card()\n",
    "#trainer.push_to_hub() \n",
    "\n",
    "# Train metrics\n",
    "metrics = train_result.metrics\n",
    "metrics['train_samples'] = len(encoded_train_dataset)\n",
    "trainer.save_model()\n",
    "trainer.log_metrics('train', metrics)\n",
    "trainer.save_metrics('train', metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "print('EVALUATING...')\n",
    "# Evaluate on labelled data\n",
    "metrics = trainer.evaluate(eval_dataset = encoded_val_dataset)\n",
    "max_eval_samples = len(encoded_val_dataset)\n",
    "metrics['eval_samples'] = max_eval_samples\n",
    "trainer.log_metrics('eval', metrics)\n",
    "trainer.save_metrics('eval', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a5e30a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTING LABELLED VALIDATION DATA...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (59512176) to match target batch_size (4).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##==================== GET PREDICTIONS & METRICS ====================##\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPREDICTING LABELLED VALIDATION DATA...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m predictions, labels, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_val_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m max_predict_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(encoded_val_dataset)\n\u001b[0;32m      5\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict_samples\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(encoded_val_dataset)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\transformers\\trainer.py:4567\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4564\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4566\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4567\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[0;32m   4569\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4570\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\transformers\\trainer.py:4685\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4682\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4684\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4685\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4686\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4687\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4689\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\transformers\\trainer.py:4902\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   4900\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m   4901\u001b[0m     num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_num_items_in_batch([inputs], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 4902\u001b[0m     loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[0;32m   4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4905\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m   4907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "Cell \u001b[1;32mIn[2], line 574\u001b[0m, in \u001b[0;36mget_weighted_trainer.<locals>._WeightedBCELossTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m    572\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    573\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([weights[\u001b[38;5;241m0\u001b[39m], weights[\u001b[38;5;241m1\u001b[39m]], device\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdtype)) \n\u001b[1;32m--> 574\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1385\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\env_dl_projeto\\lib\\site-packages\\torch\\nn\\functional.py:3458\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3457\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3459\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3462\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3465\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (59512176) to match target batch_size (4)."
     ]
    }
   ],
   "source": [
    "##==================== GET PREDICTIONS & METRICS ====================##\n",
    "print('PREDICTING LABELLED VALIDATION DATA...')\n",
    "predictions, labels, metrics = trainer.predict(encoded_val_dataset, metric_key_prefix='predict')\n",
    "max_predict_samples = len(encoded_val_dataset)\n",
    "metrics['predict_samples'] = len(encoded_val_dataset)\n",
    "trainer.log_metrics('predict', metrics)\n",
    "trainer.save_metrics('predict', metrics)\n",
    "\n",
    "preds = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Calculate performance metrics on test set\n",
    "macro_f1, micro_f1, f1_class, accuracy, precision_class, recall_class, support = calculate_metrics(labels, preds, class_names, f'{repository_id}/en_test_set_matrix.png')\n",
    "\n",
    "df_true = pd.DataFrame(labels, columns = ['True'])\n",
    "df_preds = pd.DataFrame(preds, columns = ['Prediction'])\n",
    "df_metrics = pd.DataFrame([[macro_f1, micro_f1, accuracy, f1_class, precision_class, recall_class, support]],\n",
    "                            columns = ['Macro_F1', 'Micro_F1', 'F1s', 'Accuracy', 'Precision', 'Recall', 'Support'])\n",
    "\n",
    "# Concatenate id, text, true labels and predicted labels\n",
    "final_true_preds = pd.concat([df_true, df_preds], axis = 1)\n",
    "\n",
    "final_true_preds.to_csv(f'{repository_id}/subtaskA_hate_True_Predictions.csv', encoding = 'utf-8', index = False, header = True, sep =',')\n",
    "df_metrics.to_csv(f'{repository_id}/subtaskA_hate_Results_Metrics.csv', encoding = 'utf-8', index = False, header = True, sep =',')\n",
    "\n",
    "# print('PREDICTING UNLABELLED VALIDATION DATA...')\n",
    "# unlabelled_val_predictions = trainer.predict(encoded_val_un_dataset)\n",
    "\n",
    "# # Access the predictions\n",
    "# unlabelled_val_predictions = unlabelled_val_predictions.predictions\n",
    "# unlabelled_val_predictions_preds = np.argmax(unlabelled_val_predictions, axis=-1)\n",
    "# unlabelled_val_predictions_preds = unlabelled_val_predictions_preds.flatten()\n",
    "# df_unlabelled_val_predictions = pd.DataFrame(unlabelled_val_predictions_preds, columns = ['Label_pred'])\n",
    "\n",
    "# # Create a list of dictionaries for the submission\n",
    "# submission_dict = [{\"index\": idx, \"prediction\": pred} for idx, pred in zip(unlabelled_val_data['index'], df_unlabelled_val_predictions['Label_pred'])]\n",
    "# # Sort the list of dictionaries by 'index' in ascending order\n",
    "# submission_dict_sorted = sorted(submission_dict, key=lambda x: x['index'])\n",
    "# # Write to a JSON file\n",
    "# with open(f'{repository_id}/submission_subtaskA_val.json', 'w') as f:\n",
    "#   for item in submission_dict_sorted: \n",
    "#     f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print('PREDICTING UNLABELLED TEST DATA...')\n",
    "test_predictions = trainer.predict(encoded_test_dataset)\n",
    "\n",
    "# Access the predictions\n",
    "test_predictions = test_predictions.predictions\n",
    "test_preds = np.argmax(test_predictions, axis=-1)\n",
    "test_preds = test_preds.flatten()\n",
    "df_test_predictions = pd.DataFrame(test_preds, columns = ['Label_pred'])\n",
    "\n",
    "test_data = test_data.reset_index(drop=False)\n",
    "# Create a list of dictionaries for the submission\n",
    "submission_dict_test = [{\"index\": idx, \"prediction\": pred} for idx, pred in zip(test_data['index'], df_test_predictions['Label_pred'])]\n",
    "# Sort the list of dictionaries by 'index' in ascending order\n",
    "submission_dict_sorted_test = sorted(submission_dict_test, key=lambda x: x['index'])\n",
    "# Write to a JSON file\n",
    "with open(f'{repository_id}/submission_subtaskA_test.json', 'w') as f:\n",
    "  for item in submission_dict_sorted_test: \n",
    "    f.write(json.dumps(item) + '\\n') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl_projeto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
